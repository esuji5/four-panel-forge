#!/usr/bin/env python3
"""
å¹ãå‡ºã—æ¤œå‡ºã®çµ±åˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
Fun Annotatorã‚·ã‚¹ãƒ†ãƒ ã«å¹ãå‡ºã—æ¤œå‡ºæ©Ÿèƒ½ã‚’è¿½åŠ 
"""

import base64
import io
import json
import logging
import time
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import cv2
import numpy as np
import torch
import torch.nn as nn
from torchvision import transforms
from PIL import Image
from ultralytics import YOLO

logger = logging.getLogger(__name__)

# å¹ãå‡ºã—ã‚¯ãƒ©ã‚¹ã®å®šç¾©
# YOLOãƒ¢ãƒ‡ãƒ«(yolo11x_balloon_model)ã®dataset.yamlã¨åŒæœŸ
BALLOON_CLASSES = {
    0: "speech_bubble",  # é€šå¸¸ã®å¹ãå‡ºã—
    1: "thought_bubble",  # æ€è€ƒã®å¹ãå‡ºã—
    2: "exclamation_bubble",  # æ„Ÿå˜†ç¬¦ã®å¹ãå‡ºã—
    3: "combined_bubble",  # çµåˆå‹å¹ãå‡ºã—
    4: "offserif_bubble",  # ã‚ªãƒ•ã‚»ãƒªãƒ•ï¼ˆç”»é¢å¤–ã®å£°ï¼‰
    5: "inner_voice_bubble",  # å†…ãªã‚‹å£°ï¼ˆã¤ã¶ã‚„ãï¼‰
    6: "narration_box",  # ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒœãƒƒã‚¯ã‚¹
    7: "chractor_bubble_yuzuko",  # ã‚†ãšã“å°‚ç”¨å¹ãå‡ºã—
    8: "chractor_bubble_yukari",  # ã‚†ã‹ã‚Šå°‚ç”¨å¹ãå‡ºã—
    9: "chractor_bubble_yui",  # å”¯å°‚ç”¨å¹ãå‡ºã—
    10: "chractor_bubble_yoriko",  # ã‚ˆã‚Šã“å°‚ç”¨å¹ãå‡ºã—
    11: "chractor_bubble_chiho",  # åƒç©‚å°‚ç”¨å¹ãå‡ºã—
    12: "chractor_bubble_kei",  # æµå°‚ç”¨å¹ãå‡ºã—
    13: "chractor_bubble_fumi",  # å²å°‚ç”¨å¹ãå‡ºã—
}

# å°»å°¾å½¢çŠ¶åˆ†é¡ã®ã‚«ãƒ†ã‚´ãƒª
# å°»å°¾å½¢çŠ¶åˆ†é¡ã®å…¨ã‚«ãƒ†ã‚´ãƒªï¼ˆãƒ¢ãƒ‡ãƒ«ç”¨ï¼‰
TAIL_SHAPE_CATEGORIES = [
    "ã—ã£ã½ã˜ã‚ƒãªã„",
    "ã‚ªãƒ•ã‚»ãƒªãƒ•",
    "æ€è€ƒ",
    "çœŸä¸Š",
    "çœŸä¸‹",
    "ä¸Šå·¦30åº¦ä»¥ä¸Š",
    "ä¸Šå·¦å°‘ã—",
    "ä¸Šå·¦ã‚„ã‚„",
    "ä¸Šå³ã‚„ã‚„",
    "ä¸Šå³å°‘ã—",
    "ä¸Šå³30åº¦ä»¥ä¸Š",
    "ä¸‹å·¦30åº¦ä»¥ä¸Š",
    "ä¸‹å·¦å°‘ã—",
    "ä¸‹å·¦ã‚„ã‚„",
    "ä¸‹å³ã‚„ã‚„",
    "ä¸‹å³å°‘ã—",
    "ä¸‹å³30åº¦ä»¥ä¸Š"
]

# é™¤å¤–ã™ã‚‹åˆ†é¡ï¼ˆè¡¨ç¤ºã—ãªã„ï¼‰
EXCLUDED_TAIL_CATEGORIES = ["ã—ã£ã½ã˜ã‚ƒãªã„"]


class DINOv2Classifier(nn.Module):
    """DINOv2ã‚’ä½¿ã£ãŸå°»å°¾å½¢çŠ¶åˆ†é¡å™¨"""
    
    def __init__(self, num_classes=17, model_name='dinov2_vits14', freeze_backbone=True):
        super().__init__()
        
        # DINOv2ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        self.backbone = torch.hub.load('facebookresearch/dinov2', model_name)
        
        # ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å›ºå®šã™ã‚‹ã‹
        if freeze_backbone:
            for param in self.backbone.parameters():
                param.requires_grad = False
        
        # åˆ†é¡ãƒ˜ãƒƒãƒ‰
        embed_dim = self.backbone.embed_dim
        self.classifier = nn.Sequential(
            nn.Linear(embed_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(256, num_classes)
        )
    
    def forward(self, x):
        # DINOv2ã§ç‰¹å¾´æŠ½å‡º
        features = self.backbone(x)
        # åˆ†é¡
        logits = self.classifier(features)
        return logits


class TailShapeClassifier:
    """å°»å°¾å½¢çŠ¶åˆ†é¡å™¨"""
    
    def __init__(self, model_path: str = None, device: str = 'auto'):
        """
        åˆæœŸåŒ–
        
        Args:
            model_path: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
            device: ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹
        """
        # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
        if device == 'auto':
            if torch.cuda.is_available():
                self.device = torch.device('cuda')
            elif torch.backends.mps.is_available():
                self.device = torch.device('mps')
            else:
                self.device = torch.device('cpu')
        else:
            self.device = torch.device(device)
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹
        if model_path is None:
            # æœ€æ–°ã®resultsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¢ã™
            results_dirs = list(Path(".").glob("tail_shape_dinov2_results_*"))
            if results_dirs:
                latest_dir = max(results_dirs, key=lambda x: x.stat().st_mtime)
                model_path = str(latest_dir / "models" / "best_model.pth")
            else:
                logger.warning("å°»å°¾å½¢çŠ¶åˆ†é¡ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                self.model = None
                self.is_loaded = False
                return
        
        self.model_path = model_path
        self.model = None
        self.is_loaded = False
        
        # ã‚«ãƒ†ã‚´ãƒª
        self.categories = TAIL_SHAPE_CATEGORIES
        
        # å‰å‡¦ç†è¨­å®š
        self.transform = transforms.Compose([
            transforms.Resize((98, 98), interpolation=Image.LANCZOS),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        self._load_model()
    
    def _load_model(self):
        """ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
        try:
            if not Path(self.model_path).exists():
                logger.warning(f"å°»å°¾å½¢çŠ¶åˆ†é¡ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.model_path}")
                return
            
            checkpoint = torch.load(self.model_path, map_location=self.device)
            
            # è¨­å®šã‚’å–å¾—
            config = checkpoint.get('config', {})
            model_name = config.get('model_name', 'dinov2_vits14')
            
            # ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
            self.model = DINOv2Classifier(
                num_classes=len(self.categories),
                model_name=model_name,
                freeze_backbone=True
            )
            
            # é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model = self.model.to(self.device)
            self.model.eval()
            
            self.is_loaded = True
            logger.info(f"âœ… å°»å°¾å½¢çŠ¶åˆ†é¡ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {self.model_path}")
            
        except Exception as e:
            logger.error(f"âŒ å°»å°¾å½¢çŠ¶åˆ†é¡ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            self.model = None
            self.is_loaded = False
    
    def classify_tail_shape(self, tail_image: np.ndarray) -> Dict[str, Any]:
        """
        å°»å°¾å½¢çŠ¶ã‚’åˆ†é¡
        
        Args:
            tail_image: å°»å°¾ç”»åƒï¼ˆnumpy arrayï¼‰
            
        Returns:
            åˆ†é¡çµæœ
        """
        if not self.is_loaded or self.model is None:
            return {
                'error': 'Model not loaded',
                'predicted_category': 'unknown',
                'confidence': 0.0,
                'top3_predictions': []
            }
        
        try:
            # PIL Imageã«å¤‰æ›
            if len(tail_image.shape) == 3:
                # RGB
                pil_image = Image.fromarray(tail_image)
            else:
                # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«
                pil_image = Image.fromarray(tail_image).convert('RGB')
            
            # å‰å‡¦ç†
            input_tensor = self.transform(pil_image).unsqueeze(0).to(self.device)
            
            # äºˆæ¸¬
            with torch.no_grad():
                outputs = self.model(input_tensor)
                probabilities = torch.softmax(outputs, dim=1)
                confidence, predicted = torch.max(probabilities, 1)
            
            predicted_idx = predicted.item()
            confidence_score = confidence.item()
            
            # ä¸Šä½3ã¤ã®äºˆæ¸¬ã‚’å–å¾—
            top3_probs, top3_indices = torch.topk(probabilities[0], 3)
            top3_predictions = []
            for prob, idx in zip(top3_probs, top3_indices):
                top3_predictions.append({
                    'category': self.categories[idx],
                    'confidence': prob.item()
                })
            
            return {
                'predicted_category': self.categories[predicted_idx],
                'confidence': confidence_score,
                'top3_predictions': top3_predictions
            }
            
        except Exception as e:
            logger.error(f"å°»å°¾å½¢çŠ¶åˆ†é¡ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'error': str(e),
                'predicted_category': 'unknown',
                'confidence': 0.0,
                'top3_predictions': []
            }



class BalloonDetector:
    """å¹ãå‡ºã—æ¤œå‡ºå™¨"""

    def __init__(
        self,
        model_path: str = "/Users/esuji/work/fun_annotator/yolo11x_balloon_model/best.pt",
    ):
        """
        åˆæœŸåŒ–

        Args:
            model_path: å¹ãå‡ºã—æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
        """
        self.model_path = model_path
        self.model = None
        self._current_characters = None  # ä½ç½®ãƒ™ãƒ¼ã‚¹è©±è€…æ¨å®šç”¨ã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æƒ…å ±
        self._load_model()

    def _load_model(self):
        """ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿"""
        try:
            if Path(self.model_path).exists():
                self.model = YOLO(self.model_path)
                logger.info(f"âœ… å¹ãå‡ºã—æ¤œå‡ºãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {self.model_path}")
            else:
                logger.warning(
                    f"âš ï¸ å¹ãå‡ºã—æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.model_path}"
                )
        except Exception as e:
            logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

    def _calculate_iou(self, bbox1: Dict, bbox2: Dict) -> float:
        """
        2ã¤ã®ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã®IoUï¼ˆIntersection over Unionï¼‰ã‚’è¨ˆç®—
        
        Args:
            bbox1, bbox2: {"x1": float, "y1": float, "x2": float, "y2": float}
            
        Returns:
            IoUå€¤ (0.0-1.0)
        """
        x1_1, y1_1, x2_1, y2_1 = bbox1["x1"], bbox1["y1"], bbox1["x2"], bbox1["y2"]
        x1_2, y1_2, x2_2, y2_2 = bbox2["x1"], bbox2["y1"], bbox2["x2"], bbox2["y2"]
        
        # é‡è¤‡éƒ¨åˆ†ã®åº§æ¨™ã‚’è¨ˆç®—
        x1_inter = max(x1_1, x1_2)
        y1_inter = max(y1_1, y1_2)
        x2_inter = min(x2_1, x2_2)
        y2_inter = min(y2_1, y2_2)
        
        # é‡è¤‡ãŒãªã„å ´åˆ
        if x1_inter >= x2_inter or y1_inter >= y2_inter:
            return 0.0
        
        # é‡è¤‡é¢ç©
        inter_area = (x2_inter - x1_inter) * (y2_inter - y1_inter)
        
        # å„ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã®é¢ç©
        area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
        area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
        
        # Unioné¢ç©
        union_area = area1 + area2 - inter_area
        
        return inter_area / union_area if union_area > 0 else 0.0

    def _get_balloon_priority(self, balloon_type: str) -> int:
        """
        å¹ãå‡ºã—ã‚¿ã‚¤ãƒ—ã®å„ªå…ˆåº¦ã‚’å–å¾—ï¼ˆæ•°å€¤ãŒå°ã•ã„ã»ã©é«˜å„ªå…ˆåº¦ï¼‰
        
        Args:
            balloon_type: å¹ãå‡ºã—ã‚¿ã‚¤ãƒ—
            
        Returns:
            å„ªå…ˆåº¦ï¼ˆæ•°å€¤ï¼‰
        """
        priority_map = {
            "speech_bubble": 1,                 # æœ€å„ªå…ˆ
            "thought_bubble": 2,
            "exclamation_bubble": 3,
            "combined_bubble": 4,
            "narration_box": 5,
            "inner_voice_bubble": 6,
            "chractor_bubble_yuzuko": 7,
            "chractor_bubble_yukari": 7,
            "chractor_bubble_yui": 7,
            "chractor_bubble_yoriko": 7,
            "chractor_bubble_chiho": 7,
            "chractor_bubble_kei": 7,
            "chractor_bubble_fumi": 7,
            "offserif_bubble": 10,              # æœ€ä½å„ªå…ˆåº¦
        }
        return priority_map.get(balloon_type, 8)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ä¸­ç¨‹åº¦

    def _remove_overlapping_balloons(self, detections: List[Dict[str, Any]], iou_threshold: float = 0.3) -> List[Dict[str, Any]]:
        """
        é‡è¤‡ã™ã‚‹å¹ãå‡ºã—æ¤œå‡ºã‚’é™¤å»ï¼ˆå„ªå…ˆåº¦ãƒ™ãƒ¼ã‚¹ï¼‰
        
        Args:
            detections: æ¤œå‡ºçµæœã®ãƒªã‚¹ãƒˆ
            iou_threshold: IoUé–¾å€¤ï¼ˆã“ã‚Œä»¥ä¸Šãªã‚‰é‡è¤‡ã¨ã¿ãªã™ï¼‰
            
        Returns:
            é‡è¤‡é™¤å»å¾Œã®æ¤œå‡ºçµæœãƒªã‚¹ãƒˆ
        """
        if len(detections) <= 1:
            return detections
        
        # ä¿¡é ¼åº¦ã§ã‚½ãƒ¼ãƒˆï¼ˆé«˜ã„é †ï¼‰
        sorted_detections = sorted(detections, key=lambda d: d["confidence"], reverse=True)
        
        # é‡è¤‡é™¤å»
        keep_detections = []
        for current in sorted_detections:
            is_duplicate = False
            current_bbox = current["boundingBox"]
            current_type = current["type"]
            current_priority = self._get_balloon_priority(current_type)
            
            for kept in keep_detections:
                kept_bbox = kept["boundingBox"]
                kept_type = kept["type"]
                kept_priority = self._get_balloon_priority(kept_type)
                
                iou = self._calculate_iou(current_bbox, kept_bbox)
                
                if iou > iou_threshold:
                    # é‡è¤‡æ¤œå‡º: å„ªå…ˆåº¦ã§æ±ºå®š
                    if current_priority < kept_priority:
                        # ç¾åœ¨ã®æ–¹ãŒé«˜å„ªå…ˆåº¦ â†’ æ—¢å­˜ã‚’é™¤å»ã—ã¦ç¾åœ¨ã‚’æ¡ç”¨
                        keep_detections.remove(kept)
                        logger.info(f"ğŸ”„ é‡è¤‡é™¤å»: {kept_type}({kept['confidence']:.3f}) â†’ {current_type}({current['confidence']:.3f}) (IoU: {iou:.3f})")
                        break
                    else:
                        # æ—¢å­˜ã®æ–¹ãŒé«˜å„ªå…ˆåº¦ â†’ ç¾åœ¨ã‚’é™¤å¤–
                        is_duplicate = True
                        logger.info(f"âŒ é‡è¤‡é™¤å»: {current_type}({current['confidence']:.3f}) â† {kept_type}({kept['confidence']:.3f}) (IoU: {iou:.3f})")
                        break
            
            if not is_duplicate:
                keep_detections.append(current)
        
        if len(keep_detections) != len(detections):
            logger.info(f"ğŸ¯ å¹ãå‡ºã—é‡è¤‡é™¤å»å®Œäº†: {len(detections)} -> {len(keep_detections)}å€‹")
        
        return keep_detections

    def detect_balloons(
        self,
        image: np.ndarray,
        confidence_threshold: float = 0.25,
        max_det: int = 300,
        detect_tails: bool = True,
    ) -> List[Dict[str, Any]]:
        """
        ç”»åƒã‹ã‚‰å¹ãå‡ºã—ã‚’æ¤œå‡º

        Args:
            image: æ¤œå‡ºå¯¾è±¡ã®ç”»åƒï¼ˆnumpy arrayï¼‰
            confidence_threshold: æ¤œå‡ºé–¾å€¤
            max_det: æœ€å¤§æ¤œå‡ºæ•°
            detect_tails: ã—ã£ã½æ¤œå‡ºã‚’è¡Œã†ã‹ã©ã†ã‹

        Returns:
            æ¤œå‡ºçµæœã®ãƒªã‚¹ãƒˆ
        """
        if self.model is None:
            logger.warning("ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            return []

        try:
            # YOLOæ¤œå‡ºå®Ÿè¡Œ
            results = self.model(image, conf=confidence_threshold, max_det=max_det)

            # ã—ã£ã½æ¤œå‡ºå™¨ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å–å¾—ï¼ˆå¿…è¦ãªå ´åˆã®ã¿ï¼‰
            tail_detector = get_tail_detector() if detect_tails else None

            detections = []
            for r in results:
                if r.boxes is not None:
                    for i, box in enumerate(r.boxes):
                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                        conf = box.conf[0].cpu().numpy()
                        cls = int(box.cls[0].cpu().numpy())

                        # å¹ãå‡ºã—ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
                        balloon_type = BALLOON_CLASSES.get(cls, "unknown")

                        # ã‚¯ãƒ©ã‚¹ã”ã¨ã«ç•°ãªã‚‹ä¿¡é ¼åº¦é–¾å€¤ã‚’é©ç”¨
                        # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å°‚ç”¨å¹ãå‡ºã—ã¨ã‚ªãƒ•ã‚»ãƒªãƒ•ã¯é€šå¸¸ã®é–¾å€¤
                        # ãã®ä»–ã®ä¸€èˆ¬çš„ãªå¹ãå‡ºã—ã¯ä½ã„é–¾å€¤ã§æ¤œå‡º
                        class_specific_threshold = confidence_threshold
                        if balloon_type in [
                            "speech_bubble",
                            "thought_bubble",
                            "exclamation_bubble",
                            "combined_bubble",
                            "narration_box",
                        ]:
                            # ä¸€èˆ¬çš„ãªå¹ãå‡ºã—ã¯é–¾å€¤ã‚’ä¸‹ã’ã¦ç©æ¥µçš„ã«æ¤œå‡º
                            class_specific_threshold = min(
                                0.1, confidence_threshold * 0.4
                            )
                        elif (
                            balloon_type.startswith("chractor_bubble_")
                            or balloon_type == "offserif_bubble"
                        ):
                            # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å°‚ç”¨ã¨ã‚ªãƒ•ã‚»ãƒªãƒ•ã¯é€šå¸¸ã®é–¾å€¤
                            class_specific_threshold = confidence_threshold

                        # é–¾å€¤ãƒã‚§ãƒƒã‚¯
                        if conf < class_specific_threshold:
                            continue

                        # ä¸­å¿ƒåº§æ¨™ã‚’è¨ˆç®—ï¼ˆæ­£è¦åŒ–ï¼‰
                        center_x = (x1 + x2) / 2 / image.shape[1]
                        center_y = (y1 + y2) / 2 / image.shape[0]

                        detection = {
                            "dialogueId": f"balloon_{i + 1}",
                            "type": balloon_type,
                            "boundingBox": {
                                "x1": float(x1),
                                "y1": float(y1),
                                "x2": float(x2),
                                "y2": float(y2),
                                "width": float(x2 - x1),
                                "height": float(y2 - y1),
                            },
                            "coordinate": [float(center_x), float(center_y)],
                            "confidence": float(conf),
                            "classId": cls,
                            "readingOrderIndex": i + 1,  # æš«å®šçš„ã«æ¤œå‡ºé †ã‚’èª­ã¿é †ã¨ã™ã‚‹
                        }

                        # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å°‚ç”¨å¹ãå‡ºã—ã®å ´åˆã€è©±è€…ã‚’æ¨å®šï¼ˆchar_Xå½¢å¼ã«å¤‰æ›ï¼‰
                        if balloon_type.startswith("chractor_bubble_"):
                            character_name = balloon_type.replace(
                                "chractor_bubble_", ""
                            )
                            # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åã‚’char_Xå½¢å¼ã«ãƒãƒƒãƒ”ãƒ³ã‚°
                            character_id_map = {
                                "yuzuko": "char_A",
                                "yukari": "char_B",
                                "yui": "char_C",
                                "yoriko": "char_D",
                                "chiho": "char_E",
                                "kei": "char_F",
                                "fumi": "char_G",
                            }
                            detection["speakerCharacterId"] = character_id_map.get(
                                character_name, None
                            )

                        # å…¨ã¦ã®å¹ãå‡ºã—ã‚¿ã‚¤ãƒ—ã«ã¤ã„ã¦ãƒ­ã‚°å‡ºåŠ›
                        print(f"ğŸ¯ å¹ãå‡ºã—å‡¦ç†ä¸­: {balloon_type} (ID: {detection.get('dialogueId', 'unknown')})")
                        
                        # ã—ã£ã½æ¤œå‡ºã‚’å®Ÿè¡Œï¼ˆå¯¾è±¡ã®å¹ãå‡ºã—ã‚¿ã‚¤ãƒ—ã®å ´åˆã®ã¿ï¼‰
                        print(f"ğŸ” ã—ã£ã½æ¤œå‡ºæ¡ä»¶ãƒã‚§ãƒƒã‚¯: {balloon_type}")
                        print(f"  - detect_tails: {detect_tails}")
                        print(f"  - tail_detector: {tail_detector is not None}")
                        print(f"  - balloon_type not in offserif_bubble: {balloon_type not in ['offserif_bubble']}")
                        print(f"  - not startswith chractor_bubble_: {not balloon_type.startswith('chractor_bubble_')}")
                        
                        if (
                            detect_tails
                            and tail_detector
                            and balloon_type
                            not in ["offserif_bubble"]
                            and not balloon_type.startswith("chractor_bubble_")
                        ):
                            # å¹ãå‡ºã—éƒ¨åˆ†ã®ç”»åƒã‚’åˆ‡ã‚Šå‡ºã—
                            x1_int, y1_int = max(0, int(x1)), max(0, int(y1))
                            x2_int, y2_int = (
                                min(image.shape[1], int(x2)),
                                min(image.shape[0], int(y2)),
                            )
                            balloon_image = image[y1_int:y2_int, x1_int:x2_int]

                            if balloon_image.size > 0:
                                print(f"ğŸ” {balloon_type}ã§ã—ã£ã½æ¤œå‡ºå®Ÿè¡Œä¸­... ç”»åƒã‚µã‚¤ã‚º: {balloon_image.shape}")
                                
                                # ã—ã£ã½æ¤œå‡ºï¼ˆå…¨å¹ãå‡ºã—ç¨®é¡ã§æ®µéšçš„é–¾å€¤èª¿æ•´ï¼‰
                                tail_detections = None
                                
                                # å¹ãå‡ºã—ç¨®é¡ã«å¿œã˜ãŸæ®µéšçš„é–¾å€¤è¨­å®š
                                if balloon_type == 'exclamation_bubble':
                                    # æ„Ÿå˜†å¹ãå‡ºã—ï¼šæœ€ã‚‚æ®µéšçš„ï¼ˆã—ã£ã½ãŒå°‘ãªã„ãŸã‚ï¼‰
                                    thresholds = [0.25, 0.15, 0.1, 0.05, 0.01]
                                    threshold_labels = ["æ¨™æº–", "ä½", "éå¸¸ã«ä½", "æ¥µä½", "æœ€ä½"]
                                    balloon_type_label = "æ„Ÿå˜†å¹ãå‡ºã—"
                                else:
                                    # é€šå¸¸ã®å¹ãå‡ºã—ï¼šã‚„ã‚„ä¿å®ˆçš„ãªæ®µéšçš„èª¿æ•´
                                    thresholds = [0.25, 0.2, 0.15, 0.1, 0.05]
                                    threshold_labels = ["æ¨™æº–", "ã‚„ã‚„ä½", "ä½", "éå¸¸ã«ä½", "æ¥µä½"]
                                    balloon_type_label = "é€šå¸¸å¹ãå‡ºã—"
                                
                                print(f"ğŸ”„ {balloon_type_label}ã§æ®µéšçš„ã—ã£ã½æ¤œå‡ºã‚’é–‹å§‹")
                                
                                for i, threshold in enumerate(thresholds):
                                    print(f"ğŸ“Š æ®µéš{i+1}: ä¿¡é ¼åº¦é–¾å€¤ {threshold} ({threshold_labels[i]})")
                                    
                                    tail_detections = tail_detector.detect_tails(
                                        balloon_image,
                                        balloon_type,
                                        confidence_threshold=threshold,
                                    )
                                    
                                    if tail_detections:
                                        print(f"ğŸ¯ æ®µéš{i+1}ã§ã—ã£ã½æ¤œå‡ºæˆåŠŸ: {len(tail_detections)}å€‹ (é–¾å€¤: {threshold})")
                                        break
                                    else:
                                        print(f"âŒ æ®µéš{i+1}ã§ã¯ã—ã£ã½æœªæ¤œå‡º (é–¾å€¤: {threshold})")
                                
                                print(f"ğŸ¯ æœ€çµ‚ã—ã£ã½æ¤œå‡ºçµæœ: {len(tail_detections) if tail_detections else 0}å€‹")

                                # ã—ã£ã½æƒ…å ±ã‚’è¿½åŠ ï¼ˆå…ƒç”»åƒã®åº§æ¨™ã«å¤‰æ›ï¼‰
                                if tail_detections:
                                    detection["tails"] = []
                                    
                                    for tail in tail_detections:
                                        # ã—ã£ã½ã®åº§æ¨™ã‚’å…ƒç”»åƒã®åº§æ¨™ç³»ã«å¤‰æ›
                                        tail_global = {
                                            **tail,
                                            "globalBoundingBox": {
                                                "x1": x1_int
                                                + tail["boundingBox"]["x1"],
                                                "y1": y1_int
                                                + tail["boundingBox"]["y1"],
                                                "x2": x1_int
                                                + tail["boundingBox"]["x2"],
                                                "y2": y1_int
                                                + tail["boundingBox"]["y2"],
                                            },
                                            "globalPosition": [
                                                (
                                                    x1_int
                                                    + (
                                                        tail["boundingBox"]["x1"]
                                                        + tail["boundingBox"]["x2"]
                                                    )
                                                    / 2
                                                )
                                                / image.shape[1],
                                                (
                                                    y1_int
                                                    + (
                                                        tail["boundingBox"]["y1"]
                                                        + tail["boundingBox"]["y2"]
                                                    )
                                                    / 2
                                                )
                                                / image.shape[0],
                                            ],
                                        }
                                        
                                        detection["tails"].append(tail_global)
                                else:
                                    print(f"âŒ {balloon_type}: ã—ã£ã½ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                                    
                                    # å…¨ã¦ã®å¹ãå‡ºã—ã§ã—ã£ã½ãŒæ¤œå‡ºã•ã‚Œãªã„å ´åˆã€ä½ç½®é–¢ä¿‚ã«ã‚ˆã‚‹è©±è€…æ¨å®šã‚’å®Ÿè¡Œ
                                    if True:  # å…¨ã¦ã®å¹ãå‡ºã—ç¨®é¡ã§ä½ç½®ãƒ™ãƒ¼ã‚¹æ¨å®šã‚’å®Ÿè¡Œ
                                        print(f"ğŸ¯ {balloon_type_label}ã§ã—ã£ã½æœªæ¤œå‡ºã®ãŸã‚ä½ç½®é–¢ä¿‚ã«ã‚ˆã‚‹è©±è€…æ¨å®šã‚’å®Ÿè¡Œ")
                                        
                                        # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æ¤œå‡ºçµæœãŒã‚ã‚Œã°ä½ç½®ãƒ™ãƒ¼ã‚¹æ¨å®šã‚’å®Ÿè¡Œ
                                        if hasattr(self, '_current_characters') and self._current_characters:
                                            speaker_result = self._estimate_speaker_by_proximity(detection, self._current_characters)
                                            if speaker_result:
                                                speaker_id = speaker_result["speaker_id"]
                                                confidence = speaker_result["confidence"]
                                                distance = speaker_result["distance"]
                                                confidence_level = speaker_result["confidence_level"]
                                                stage = speaker_result["threshold_stage"]
                                                
                                                print(f"ğŸ“ ä½ç½®é–¢ä¿‚ã«ã‚ˆã‚Šè©±è€…æ¨å®š: {speaker_id} (æ®µéš{stage}, {confidence_level})")
                                                
                                                # ãƒ€ãƒŸãƒ¼ã®ã—ã£ã½æƒ…å ±ã‚’ä½œæˆï¼ˆä½ç½®é–¢ä¿‚ãƒ™ãƒ¼ã‚¹ã€ä¿¡é ¼åº¦åæ˜ ï¼‰
                                                dummy_tail = {
                                                    "boundingBox": detection["boundingBox"],
                                                    "globalBoundingBox": detection["boundingBox"],
                                                    "direction": "position_based",
                                                    "category": f"ä½ç½®ãƒ™ãƒ¼ã‚¹æ¨å®š({confidence_level})",
                                                    "confidence": confidence,
                                                    "distance": distance,
                                                    "position_based": True,
                                                    "estimated_speaker": speaker_id,
                                                    "threshold_stage": stage
                                                }
                                                detection["tails"].append(dummy_tail)
                                                detection["position_based_speaker"] = True
                                            else:
                                                print(f"ğŸš« ä½ç½®é–¢ä¿‚ã«ã‚ˆã‚‹è©±è€…æ¨å®šã«å¤±æ•—")
                                        else:
                                            print(f"âš ï¸ ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æƒ…å ±ãŒãªã„ãŸã‚ä½ç½®ãƒ™ãƒ¼ã‚¹æ¨å®šã‚’ã‚¹ã‚­ãƒƒãƒ—")
                            else:
                                print(f"âš ï¸ {balloon_type}: å¹ãå‡ºã—ç”»åƒãŒç©ºã§ã™")
                        else:
                            print(f"ğŸš« {balloon_type}: ã—ã£ã½æ¤œå‡ºæ¡ä»¶ã‚’æº€ãŸã•ãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")

                        detections.append(detection)

            # é‡è¤‡ã™ã‚‹å¹ãå‡ºã—æ¤œå‡ºã‚’é™¤å»ï¼ˆå„ªå…ˆåº¦ãƒ™ãƒ¼ã‚¹ï¼‰
            if len(detections) > 1:
                detections = self._remove_overlapping_balloons(detections, iou_threshold=0.3)

            # å³ä¸Šã‹ã‚‰å·¦ä¸‹ã¸ã®èª­ã¿é †ã§ã‚½ãƒ¼ãƒˆï¼ˆæ—¥æœ¬ã®æ¼«ç”»ã®ä¸€èˆ¬çš„ãªèª­ã¿æ–¹ï¼‰
            # ã‚ˆã‚Šè©³ç´°ãªèª­ã¿é †ãƒ­ã‚¸ãƒƒã‚¯ï¼š
            # 1. ã¾ãšå¤§ã¾ã‹ãªè¡Œï¼ˆYåº§æ¨™ã‚’åŸºæº–ï¼‰ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            # 2. å„è¡Œå†…ã§å³ã‹ã‚‰å·¦ã«ã‚½ãƒ¼ãƒˆ

            # Yåº§æ¨™ã®é–¾å€¤ï¼ˆç”»åƒé«˜ã•ã®10%ä»¥å†…ãªã‚‰åŒã˜è¡Œã¨ã¿ãªã™ï¼‰
            y_threshold = 0.1

            # è¡Œã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            rows = []
            for detection in detections:
                y = detection["coordinate"][1]
                placed = False
                for row in rows:
                    if any(abs(y - d["coordinate"][1]) < y_threshold for d in row):
                        row.append(detection)
                        placed = True
                        break
                if not placed:
                    rows.append([detection])

            # å„è¡Œã‚’ä¸Šã‹ã‚‰ä¸‹ã«ã€è¡Œå†…ã¯å³ã‹ã‚‰å·¦ã«ã‚½ãƒ¼ãƒˆ
            rows.sort(key=lambda row: min(d["coordinate"][1] for d in row))
            sorted_detections = []
            for row in rows:
                row.sort(key=lambda d: -d["coordinate"][0])  # å³ã‹ã‚‰å·¦
                sorted_detections.extend(row)

            detections = sorted_detections

            # readingOrderIndexã‚’æ›´æ–°
            for i, detection in enumerate(detections):
                detection["readingOrderIndex"] = i + 1

            # å°»å°¾å½¢çŠ¶åˆ†é¡ã‚’å®Ÿè¡Œ
            detections = classify_tail_shapes_in_detections(image, detections)

            return detections

        except Exception as e:
            logger.error(f"å¹ãå‡ºã—æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def detect_from_base64(
        self,
        base64_image: str,
        confidence_threshold: float = 0.25,
        max_det: int = 300,
        detect_tails: bool = True,
    ) -> List[Dict[str, Any]]:
        """
        Base64ç”»åƒã‹ã‚‰å¹ãå‡ºã—ã‚’æ¤œå‡º

        Args:
            base64_image: Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒ
            confidence_threshold: æ¤œå‡ºé–¾å€¤
            max_det: æœ€å¤§æ¤œå‡ºæ•°
            detect_tails: ã—ã£ã½æ¤œå‡ºã‚’è¡Œã†ã‹ã©ã†ã‹

        Returns:
            æ¤œå‡ºçµæœã®ãƒªã‚¹ãƒˆ
        """
        try:
            # Base64ãƒ‡ã‚³ãƒ¼ãƒ‰
            if base64_image.startswith("data:image"):
                base64_image = base64_image.split(",")[1]

            image_data = base64.b64decode(base64_image)
            image = Image.open(io.BytesIO(image_data))

            # numpyé…åˆ—ã«å¤‰æ›
            image_np = np.array(image)
            if image_np.shape[2] == 4:  # RGBA -> RGB
                image_np = cv2.cvtColor(image_np, cv2.COLOR_RGBA2RGB)
            elif len(image_np.shape) == 2:  # Grayscale -> RGB
                image_np = cv2.cvtColor(image_np, cv2.COLOR_GRAY2RGB)

            return self.detect_balloons(
                image_np, confidence_threshold, max_det, detect_tails
            )

        except Exception as e:
            logger.error(f"Base64ç”»åƒã®å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def detect_from_path(
        self,
        image_path: str,
        confidence_threshold: float = 0.25,
        max_det: int = 300,
        detect_tails: bool = True,
    ) -> List[Dict[str, Any]]:
        """
        ç”»åƒãƒ‘ã‚¹ã‹ã‚‰å¹ãå‡ºã—ã‚’æ¤œå‡º

        Args:
            image_path: ç”»åƒã®ãƒ‘ã‚¹
            confidence_threshold: æ¤œå‡ºé–¾å€¤
            max_det: æœ€å¤§æ¤œå‡ºæ•°
            detect_tails: ã—ã£ã½æ¤œå‡ºã‚’è¡Œã†ã‹ã©ã†ã‹

        Returns:
            æ¤œå‡ºçµæœã®ãƒªã‚¹ãƒˆ
        """
        try:
            # ç”»åƒèª­ã¿è¾¼ã¿
            image = cv2.imread(image_path)
            if image is None:
                logger.error(f"ç”»åƒã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“: {image_path}")
                return []

            # BGRã‹ã‚‰RGBã«å¤‰æ›
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

            return self.detect_balloons(
                image, confidence_threshold, max_det, detect_tails
            )

        except Exception as e:
            logger.error(f"ç”»åƒèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def crop_and_classify_balloons(
        self,
        image_path: str,
        confidence_threshold: float = 0.25,
        max_det: int = 300,
        save_crops: bool = True,
        output_dir: str = "./tmp/balloon_crops"
    ) -> Dict[str, Any]:
        """
        å¹ãå‡ºã—ã‚’æ¤œå‡ºãƒ»åˆ‡ã‚Šå‡ºã—ãƒ»åˆ†é¡ã—ã€çµæœã‚’å¯è¦–åŒ–
        
        Args:
            image_path: ç”»åƒã®ãƒ‘ã‚¹
            confidence_threshold: æ¤œå‡ºé–¾å€¤
            max_det: æœ€å¤§æ¤œå‡ºæ•°
            save_crops: åˆ‡ã‚Šå‡ºã—ç”»åƒã‚’ä¿å­˜ã™ã‚‹ã‹ã©ã†ã‹
            output_dir: åˆ‡ã‚Šå‡ºã—ç”»åƒã®ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        
        Returns:
            æ¤œå‡ºãƒ»åˆ†é¡çµæœã¨å¯è¦–åŒ–ç”»åƒã®ãƒ‘ã‚¹
        """
        try:
            # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
            
            # å…ƒç”»åƒã‚’èª­ã¿è¾¼ã¿
            image = cv2.imread(image_path)
            if image is None:
                logger.error(f"ç”»åƒã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“: {image_path}")
                return {"error": "ç”»åƒèª­ã¿è¾¼ã¿å¤±æ•—"}
            
            # BGRã‹ã‚‰RGBã«å¤‰æ›
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # å¹ãå‡ºã—æ¤œå‡ºå®Ÿè¡Œ
            detections = self.detect_balloons(image_rgb, confidence_threshold, max_det)
            
            if not detections:
                return {
                    "detections": [],
                    "cropped_images": [],
                    "classification_results": [],
                    "visualization_path": None
                }
            
            # çµæœã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
            cropped_images = []
            classification_results = []
            
            # å„å¹ãå‡ºã—ã‚’åˆ‡ã‚Šå‡ºã—ãƒ»åˆ†é¡
            for i, detection in enumerate(detections):
                balloon_id = detection.get("dialogueId", f"balloon_{i}")
                bbox = detection.get("boundingBox", {})
                
                if not bbox:
                    continue
                
                # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‹ã‚‰åˆ‡ã‚Šå‡ºã—
                x1, y1 = int(bbox["x1"]), int(bbox["y1"])
                x2, y2 = int(bbox["x2"]), int(bbox["y2"])
                
                # åˆ‡ã‚Šå‡ºã—ç”»åƒã‚’å–å¾—
                cropped_balloon = image_rgb[y1:y2, x1:x2]
                
                if cropped_balloon.size == 0:
                    continue
                
                # åˆ‡ã‚Šå‡ºã—ç”»åƒæƒ…å ±ã‚’è¿½åŠ 
                crop_info = {
                    "balloon_id": balloon_id,
                    "bbox": bbox,
                    "type": detection.get("type", "unknown"),
                    "confidence": detection.get("confidence", 0.0),
                    "size": {
                        "width": x2 - x1,
                        "height": y2 - y1
                    }
                }
                
                # åˆ‡ã‚Šå‡ºã—ç”»åƒã‚’ä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                if save_crops:
                    crop_filename = f"{balloon_id}_{detection.get('type', 'unknown')}.png"
                    crop_path = output_path / crop_filename
                    
                    # PIL Imageã§ä¿å­˜
                    pil_image = Image.fromarray(cropped_balloon)
                    pil_image.save(crop_path)
                    crop_info["crop_path"] = str(crop_path)
                
                cropped_images.append(crop_info)
                
                # ç°¡å˜ãªåˆ†é¡ï¼ˆã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹ï¼‰
                classification = self._classify_balloon_by_features(cropped_balloon, detection)
                classification_results.append({
                    "balloon_id": balloon_id,
                    "classification": classification
                })
            
            # å¯è¦–åŒ–ç”»åƒã‚’ç”Ÿæˆ
            visualization_path = self._create_visualization(
                image_rgb, detections, classification_results, output_path
            )
            
            return {
                "detections": detections,
                "cropped_images": cropped_images,
                "classification_results": classification_results,
                "visualization_path": visualization_path,
                "total_balloons": len(detections)
            }
            
        except Exception as e:
            logger.error(f"å¹ãå‡ºã—åˆ‡ã‚Šå‡ºã—ãƒ»åˆ†é¡ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}
    
    def _classify_balloon_by_features(self, balloon_image: np.ndarray, detection: Dict) -> Dict:
        """
        å¹ãå‡ºã—ç”»åƒã®ç‰¹å¾´ã«ã‚ˆã‚‹åˆ†é¡
        
        Args:
            balloon_image: åˆ‡ã‚Šå‡ºã—ãŸå¹ãå‡ºã—ç”»åƒ
            detection: æ¤œå‡ºçµæœ
        
        Returns:
            åˆ†é¡çµæœ
        """
        try:
            height, width = balloon_image.shape[:2]
            area = height * width
            aspect_ratio = width / height if height > 0 else 0
            
            # åŸºæœ¬çš„ãªç‰¹å¾´é‡ã‚’è¨ˆç®—
            features = {
                "width": width,
                "height": height,
                "area": area,
                "aspect_ratio": aspect_ratio,
                "type": detection.get("type", "unknown"),
                "confidence": detection.get("confidence", 0.0)
            }
            
            # ã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹ã§ã®ç°¡å˜ãªåˆ†é¡
            if area < 1000:
                size_category = "small"
            elif area < 5000:
                size_category = "medium"
            else:
                size_category = "large"
            
            # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã«ã‚ˆã‚‹å½¢çŠ¶åˆ†é¡
            if aspect_ratio > 2.0:
                shape_category = "wide"
            elif aspect_ratio < 0.5:
                shape_category = "tall"
            else:
                shape_category = "normal"
            
            return {
                "features": features,
                "size_category": size_category,
                "shape_category": shape_category,
                "area_pixels": area
            }
            
        except Exception as e:
            logger.error(f"ç‰¹å¾´åˆ†é¡ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}
    
    def _create_visualization(
        self, 
        image: np.ndarray, 
        detections: List[Dict], 
        classifications: List[Dict],
        output_path: Path
    ) -> str:
        """
        æ¤œå‡ºçµæœã¨åˆ†é¡çµæœã®å¯è¦–åŒ–ç”»åƒã‚’ä½œæˆ
        
        Args:
            image: å…ƒç”»åƒ
            detections: å¹ãå‡ºã—æ¤œå‡ºçµæœ
            classifications: åˆ†é¡çµæœ
            output_path: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
        
        Returns:
            å¯è¦–åŒ–ç”»åƒã®ãƒ‘ã‚¹
        """
        try:
            # OpenCVã§æç”»ã™ã‚‹ãŸã‚ã€RGBã‹ã‚‰BGRã«å¤‰æ›
            vis_image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
            
            # åˆ†é¡çµæœã‚’IDã§ãƒãƒƒãƒ”ãƒ³ã‚°
            classification_map = {c["balloon_id"]: c for c in classifications}
            
            for i, detection in enumerate(detections):
                balloon_id = detection.get("dialogueId", f"balloon_{i}")
                bbox = detection.get("boundingBox", {})
                
                if not bbox:
                    continue
                
                x1, y1 = int(bbox["x1"]), int(bbox["y1"])
                x2, y2 = int(bbox["x2"]), int(bbox["y2"])
                
                # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’æç”»
                cv2.rectangle(vis_image, (x1, y1), (x2, y2), (0, 255, 0), 2)
                
                # ãƒ©ãƒ™ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã‚’æº–å‚™
                balloon_type = detection.get("type", "unknown")
                confidence = detection.get("confidence", 0.0)
                label = f"{balloon_type} ({confidence:.2f})"
                
                # åˆ†é¡çµæœãŒã‚ã‚Œã°è¿½åŠ 
                if balloon_id in classification_map:
                    classification = classification_map[balloon_id]["classification"]
                    size_cat = classification.get("size_category", "")
                    shape_cat = classification.get("shape_category", "")
                    label += f"\n{size_cat}/{shape_cat}"
                
                # ãƒ©ãƒ™ãƒ«ã‚’æç”»
                cv2.putText(vis_image, label, (x1, y1-10), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
                cv2.putText(vis_image, label, (x1, y1-10), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)
            
            # å¯è¦–åŒ–ç”»åƒã‚’ä¿å­˜
            timestamp = int(time.time())
            vis_filename = f"balloon_visualization_{timestamp}.png"
            vis_path = output_path / vis_filename
            
            cv2.imwrite(str(vis_path), vis_image)
            return str(vis_path)
            
        except Exception as e:
            logger.error(f"å¯è¦–åŒ–ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def _estimate_speaker_by_proximity(self, balloon_detection: Dict, characters: List[Dict]) -> Optional[str]:
        """
        ä½ç½®é–¢ä¿‚ã«ã‚ˆã‚‹è©±è€…æ¨å®šï¼ˆã—ã£ã½ãŒãªã„æ„Ÿå˜†å¹ãå‡ºã—ç”¨ï¼‰
        æ®µéšçš„é–¾å€¤èª¿æ•´ã§ç¢ºå®Ÿã«è©±è€…ã‚’è¦‹ã¤ã‘ã‚‹
        
        Args:
            balloon_detection: å¹ãå‡ºã—æ¤œå‡ºçµæœ
            characters: ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æ¤œå‡ºçµæœ
            
        Returns:
            æ¨å®šã•ã‚ŒãŸè©±è€…IDï¼ˆchar_A, char_Bç­‰ï¼‰ã¾ãŸã¯None
        """
        if not characters:
            return None
            
        balloon_center = balloon_detection.get("coordinate")
        if not balloon_center:
            return None
            
        # å„ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã¨ã®è·é›¢ã‚’è¨ˆç®—
        character_distances = []
        for char in characters:
            if not char.get("coordinate"):
                continue
                
            char_pos = char["coordinate"]
            # å¹ãå‡ºã—ã®ä¸­å¿ƒã¨ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®ä¸­å¿ƒã®è·é›¢ã‚’è¨ˆç®—
            distance = ((balloon_center[0] - char_pos[0])**2 + 
                       (balloon_center[1] - char_pos[1])**2)**0.5
            
            character_distances.append({
                "character": char,
                "distance": distance,
                "speaker_id": char.get("character") or char.get("characterId")
            })
            
            print(f"ğŸ“ {char.get('character', 'unknown')}ã¨ã®è·é›¢: {distance:.2f}")
        
        if not character_distances:
            print(f"ğŸš« æœ‰åŠ¹ãªã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
        
        # è·é›¢ã§ã‚½ãƒ¼ãƒˆï¼ˆæœ€ã‚‚è¿‘ã„ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‹ã‚‰ï¼‰
        character_distances.sort(key=lambda x: x["distance"])
        closest = character_distances[0]
        
        # æ®µéšçš„é–¾å€¤èª¿æ•´ã§è©±è€…ã‚’æ±ºå®š
        # é–¾å€¤ã®æ®µéš: [0.25, 0.4, 0.6, 0.8, 1.2] (æœ€å¾Œã¯ç”»åƒå…¨ä½“)
        thresholds = [0.25, 0.4, 0.6, 0.8, 1.2]
        confidence_labels = ["éå¸¸ã«é«˜ã„", "é«˜ã„", "ä¸­ç¨‹åº¦", "ä½ã„", "éå¸¸ã«ä½ã„"]
        confidence_values = [0.9, 0.7, 0.5, 0.3, 0.1]  # ä¿¡é ¼åº¦ã®æ•°å€¤
        
        for i, threshold in enumerate(thresholds):
            if closest["distance"] <= threshold:
                confidence_level = confidence_labels[i]
                confidence_value = confidence_values[i]
                print(f"ğŸ¯ æ®µéš{i+1}ã§è©±è€…ç™ºè¦‹: {closest['speaker_id']} (è·é›¢: {closest['distance']:.2f}, ä¿¡é ¼åº¦: {confidence_level})")
                
                # çµæœã«ä¿¡é ¼åº¦æƒ…å ±ã‚’å«ã‚ã¦è¿”ã™
                return {
                    "speaker_id": closest["speaker_id"],
                    "confidence": confidence_value,
                    "distance": closest["distance"],
                    "confidence_level": confidence_level,
                    "threshold_stage": i + 1
                }
        
        # ã©ã®é–¾å€¤ã§ã‚‚è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆï¼ˆç†è«–ä¸Šã¯èµ·ã“ã‚‰ãªã„ï¼‰
        print(f"ğŸš« ã™ã¹ã¦ã®é–¾å€¤ã§è©±è€…ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ (æœ€å°è·é›¢: {closest['distance']:.2f})")
        return None

    def set_character_information(self, characters: Optional[List[Dict]]):
        """
        ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æƒ…å ±ã‚’è¨­å®šï¼ˆä½ç½®ãƒ™ãƒ¼ã‚¹è©±è€…æ¨å®šç”¨ï¼‰
        
        Args:
            characters: ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æ¤œå‡ºçµæœ
        """
        self._current_characters = characters
        if characters:
            print(f"ğŸ’¾ ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æƒ…å ±ã‚’è¨­å®š: {len(characters)}äºº")
        else:
            print(f"ğŸ§¹ ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æƒ…å ±ã‚’ã‚¯ãƒªã‚¢")


def integrate_balloon_detection(
    panel_data: Dict,
    balloon_detections: List[Dict],
    characters: Optional[List[Dict]] = None,
) -> Dict:
    """
    å¹ãå‡ºã—æ¤œå‡ºçµæœã‚’ãƒ‘ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ã«çµ±åˆ

    Args:
        panel_data: æ—¢å­˜ã®ãƒ‘ãƒãƒ«ãƒ‡ãƒ¼ã‚¿
        balloon_detections: å¹ãå‡ºã—æ¤œå‡ºçµæœ
        characters: ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æ¤œå‡ºçµæœï¼ˆã—ã£ã½ã‹ã‚‰ã®è©±è€…æ¨å®šç”¨ï¼‰

    Returns:
        çµ±åˆã•ã‚ŒãŸãƒ‘ãƒãƒ«ãƒ‡ãƒ¼ã‚¿
    """
    # serifsãŒãªã„å ´åˆã¯åˆæœŸåŒ–
    if "serifs" not in panel_data or panel_data["serifs"] is None:
        panel_data["serifs"] = []

    # æ—¢å­˜ã®ã‚»ãƒªãƒ•ã‚’ä¿æŒã—ã¤ã¤ã€å¹ãå‡ºã—æƒ…å ±ã‚’æ›´æ–°
    existing_serifs = {s.get("dialogueId"): s for s in panel_data["serifs"]}
    detection_map = {d["dialogueId"]: d for d in balloon_detections}
    processed_ids = set()

    updated_serifs = []

    # 1. æ—¢å­˜ã®ã‚»ãƒªãƒ•ã‚’æ›´æ–°ï¼ˆæ¤œå‡ºçµæœãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰
    for serif in panel_data["serifs"]:
        dialogue_id = serif.get("dialogueId")
        processed_ids.add(dialogue_id)

        if dialogue_id in detection_map:
            detection = detection_map[dialogue_id]
            # æ—¢å­˜ã®ã‚»ãƒªãƒ•ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦æ›´æ–°
            updated_serif = serif.copy()
            # å¹ãå‡ºã—æ¤œå‡ºçµæœã§æ›´æ–°ï¼ˆã‚¿ã‚¤ãƒ—ã€åº§æ¨™ã€èª­ã¿é †ã®ã¿ï¼‰
            updated_serif["type"] = detection["type"]
            updated_serif["boundingBox"] = detection["boundingBox"]
            updated_serif["coordinate"] = detection["coordinate"]
            updated_serif["readingOrderIndex"] = detection["readingOrderIndex"]

            # ã—ã£ã½æƒ…å ±ãŒã‚ã‚Œã°è¿½åŠ 
            if "tails" in detection:
                updated_serif["tails"] = detection["tails"]

            # è©±è€…æ¨å®šï¼ˆã‚ªãƒ•ã‚»ãƒªãƒ•ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            if detection["type"] == "offserif_bubble":
                # ã‚ªãƒ•ã‚»ãƒªãƒ•ã®å ´åˆã¯è©±è€…ä¸æ˜ã®ã¾ã¾ï¼ˆè©±è€…æ¨å®šã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰
                print(f"ğŸ”• ã‚ªãƒ•ã‚»ãƒªãƒ•ã®ãŸã‚è©±è€…æ¨å®šã‚’ã‚¹ã‚­ãƒƒãƒ—: {dialogue_id}")
                updated_serif["speakerCharacterId"] = None
            elif (
                detection["type"].startswith("chractor_bubble_")
                and "speakerCharacterId" in detection
            ):
                # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å°‚ç”¨å¹ãå‡ºã—ã®å ´åˆã¯è‡ªå‹•è¨­å®š
                updated_serif["speakerCharacterId"] = detection["speakerCharacterId"]
            elif characters and "tails" in detection and detection["tails"]:
                # 16ã‚¯ãƒ©ã‚¹å°»å°¾å½¢çŠ¶ã‹ã‚‰è©±è€…ã‚’æ¨å®šï¼ˆæ”¹å–„ç‰ˆï¼‰
                speaker_id = _estimate_speaker_from_tails_16class(detection, characters)
                if speaker_id:
                    updated_serif["speakerCharacterId"] = speaker_id
            # ãã‚Œä»¥å¤–ã¯æ—¢å­˜ã®è©±è€…ã‚’ä¿æŒï¼ˆæ›´æ–°ã—ãªã„ï¼‰

            updated_serifs.append(updated_serif)
        else:
            # æ¤œå‡ºã•ã‚Œãªã‹ã£ãŸæ—¢å­˜ã‚»ãƒªãƒ•ã‚‚ãã®ã¾ã¾ä¿æŒ
            updated_serifs.append(serif)

    # 2. æ–°è¦æ¤œå‡ºã•ã‚ŒãŸå¹ãå‡ºã—ã‚’è¿½åŠ 
    for detection in balloon_detections:
        dialogue_id = detection["dialogueId"]
        if dialogue_id not in processed_ids:
            # è©±è€…æ¨å®šï¼ˆã‚ªãƒ•ã‚»ãƒªãƒ•ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            speaker_id = None
            if detection["type"] == "offserif_bubble":
                # ã‚ªãƒ•ã‚»ãƒªãƒ•ã®å ´åˆã¯è©±è€…ä¸æ˜ã®ã¾ã¾ï¼ˆè©±è€…æ¨å®šã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰
                print(f"ğŸ”• ã‚ªãƒ•ã‚»ãƒªãƒ•ã®ãŸã‚è©±è€…æ¨å®šã‚’ã‚¹ã‚­ãƒƒãƒ—: {dialogue_id}")
                speaker_id = None
            elif detection["type"].startswith("chractor_bubble_"):
                speaker_id = detection.get("speakerCharacterId", None)
            elif characters and "tails" in detection and detection["tails"]:
                speaker_id = _estimate_speaker_from_tails_16class(detection, characters)

            # æ–°è¦ã‚»ãƒªãƒ•
            serif = {
                "dialogueId": dialogue_id,
                "text": "",  # ãƒ†ã‚­ã‚¹ãƒˆã¯ç©ºã§åˆæœŸåŒ–
                "type": detection["type"],
                "speakerCharacterId": speaker_id,
                "boundingBox": detection["boundingBox"],
                "readingOrderIndex": detection["readingOrderIndex"],
                "coordinate": detection["coordinate"],
            }

            # ã—ã£ã½æƒ…å ±ãŒã‚ã‚Œã°è¿½åŠ 
            if "tails" in detection:
                serif["tails"] = detection["tails"]

            updated_serifs.append(serif)

    # èª­ã¿é †ã§ã‚½ãƒ¼ãƒˆ
    updated_serifs.sort(key=lambda s: s["readingOrderIndex"])

    panel_data["serifs"] = updated_serifs
    panel_data["serifsNum"] = len(updated_serifs)

    return panel_data


def _estimate_speaker_from_tails(
    balloon_detection: Dict, characters: List[Dict]
) -> Optional[str]:
    """
    ã—ã£ã½ã®æ–¹å‘ã‹ã‚‰è©±è€…ã‚’æ¨å®š

    Args:
        balloon_detection: å¹ãå‡ºã—æ¤œå‡ºçµæœï¼ˆã—ã£ã½æƒ…å ±å«ã‚€ï¼‰
        characters: ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æ¤œå‡ºçµæœ

    Returns:
        æ¨å®šã•ã‚ŒãŸè©±è€…IDï¼ˆchar_A, char_Bç­‰ï¼‰ã¾ãŸã¯None
    """
    # ã‚ªãƒ•ã‚»ãƒªãƒ•ã®å ´åˆã¯è©±è€…æ¨å®šã‚’ã‚¹ã‚­ãƒƒãƒ—
    if balloon_detection.get("type") == "offserif_bubble":
        print(f"ğŸ”• ã‚ªãƒ•ã‚»ãƒªãƒ•ã®ãŸã‚å¾“æ¥è©±è€…æ¨å®šã‚’ã‚¹ã‚­ãƒƒãƒ—")
        return None
    
    if not balloon_detection.get("tails") or not characters:
        return None

    # æœ€åˆã®ã—ã£ã½ã®æƒ…å ±ã‚’ä½¿ç”¨ï¼ˆé€šå¸¸ã¯1ã¤ï¼‰
    tail = balloon_detection["tails"][0]
    tail_direction = tail.get("direction", "")

    # ã—ã£ã½ã®æ–¹å‘ã‹ã‚‰æ¨å®šã•ã‚Œã‚‹ç›¸å¯¾ä½ç½®ã‚’è¨ˆç®—
    balloon_center = balloon_detection["coordinate"]

    # æ–¹å‘ã«åŸºã¥ã„ã¦è©±è€…ã®æ¨å®šä½ç½®ã‚’è¨ˆç®—
    estimated_speaker_pos = None
    if "bottom" in tail_direction:
        # ã—ã£ã½ãŒä¸‹å‘ã -> è©±è€…ã¯ä¸‹ã«ã„ã‚‹
        estimated_speaker_pos = [balloon_center[0], balloon_center[1] + 0.2]
    elif "top" in tail_direction:
        # ã—ã£ã½ãŒä¸Šå‘ã -> è©±è€…ã¯ä¸Šã«ã„ã‚‹
        estimated_speaker_pos = [balloon_center[0], balloon_center[1] - 0.2]
    elif "left" in tail_direction:
        # ã—ã£ã½ãŒå·¦å‘ã -> è©±è€…ã¯å·¦ã«ã„ã‚‹
        estimated_speaker_pos = [balloon_center[0] - 0.2, balloon_center[1]]
    elif "right" in tail_direction:
        # ã—ã£ã½ãŒå³å‘ã -> è©±è€…ã¯å³ã«ã„ã‚‹
        estimated_speaker_pos = [balloon_center[0] + 0.2, balloon_center[1]]
    else:
        # ä¸­å¤®ã¾ãŸã¯ä¸æ˜ãªå ´åˆã¯æœ€ã‚‚è¿‘ã„ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚’é¸æŠ
        estimated_speaker_pos = balloon_center

    # æœ€ã‚‚è¿‘ã„ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚’è¦‹ã¤ã‘ã‚‹
    min_distance = float("inf")
    closest_character = None

    for char in characters:
        if char.get("coordinate"):
            char_pos = char["coordinate"]
            distance = (
                (char_pos[0] - estimated_speaker_pos[0]) ** 2
                + (char_pos[1] - estimated_speaker_pos[1]) ** 2
            ) ** 0.5

            # ã—ã£ã½ã®æ–¹å‘ã¨æ•´åˆæ€§ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            direction_match = True
            if "bottom" in tail_direction and char_pos[1] < balloon_center[1]:
                direction_match = False
            elif "top" in tail_direction and char_pos[1] > balloon_center[1]:
                direction_match = False
            elif "left" in tail_direction and char_pos[0] > balloon_center[0]:
                direction_match = False
            elif "right" in tail_direction and char_pos[0] < balloon_center[0]:
                direction_match = False

            if direction_match and distance < min_distance:
                min_distance = distance
                closest_character = char

    # è·é›¢ãŒå¦¥å½“ãªç¯„å›²å†…ï¼ˆç”»é¢ã®40%ä»¥å†…ï¼‰ã§ã‚ã‚Œã°è©±è€…ã¨åˆ¤å®š
    if closest_character and min_distance < 0.4:
        return closest_character.get("character") or closest_character.get(
            "characterId"
        )

    return None


def _estimate_speaker_from_tails_16class(
    balloon_detection: Dict, characters: List[Dict]
) -> Optional[str]:
    """
    16ã‚¯ãƒ©ã‚¹å°»å°¾å½¢çŠ¶åˆ†é¡çµæœã‹ã‚‰è©±è€…ã‚’æ¨å®šï¼ˆæ”¹å–„ç‰ˆï¼‰
    
    Args:
        balloon_detection: å¹ãå‡ºã—æ¤œå‡ºçµæœï¼ˆã—ã£ã½æƒ…å ±å«ã‚€ï¼‰
        characters: ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æ¤œå‡ºçµæœ
        
    Returns:
        æ¨å®šã•ã‚ŒãŸè©±è€…IDï¼ˆchar_A, char_Bç­‰ï¼‰ã¾ãŸã¯None
    """
    # ã‚ªãƒ•ã‚»ãƒªãƒ•ã®å ´åˆã¯è©±è€…æ¨å®šã‚’ã‚¹ã‚­ãƒƒãƒ—
    if balloon_detection.get("type") == "offserif_bubble":
        print(f"ğŸ”• ã‚ªãƒ•ã‚»ãƒªãƒ•ã®ãŸã‚16ã‚¯ãƒ©ã‚¹è©±è€…æ¨å®šã‚’ã‚¹ã‚­ãƒƒãƒ—")
        return None
    
    if not balloon_detection.get("tails") or not characters:
        return None
    
    # ã—ã£ã½ã®æƒ…å ±ã‚’å–å¾—
    tail = balloon_detection["tails"][0]
    tail_shape = tail.get("shape_category", "") or tail.get("direction", "")
    
    if not tail_shape or tail_shape in ["ã—ã£ã½ã˜ã‚ƒãªã„", "ã‚ªãƒ•ã‚»ãƒªãƒ•", "unknown"]:
        # ã‚ªãƒ•ã‚»ãƒªãƒ•åˆ†é¡ã®å ´åˆã¯æ˜ç¤ºçš„ã«è©±è€…æ¨å®šã‚’ã‚¹ã‚­ãƒƒãƒ—
        if tail_shape == "ã‚ªãƒ•ã‚»ãƒªãƒ•":
            print(f"ğŸ”• ã—ã£ã½åˆ†é¡ãŒã€Œã‚ªãƒ•ã‚»ãƒªãƒ•ã€ã®ãŸã‚è©±è€…æ¨å®šã‚’ã‚¹ã‚­ãƒƒãƒ—")
            return None
        # ãã®ä»–ã®åˆ†é¡ã§ããªã„å ´åˆã¯å¾“æ¥ã®æ–¹æ³•ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return _estimate_speaker_from_tails(balloon_detection, characters)
    
    balloon_center = balloon_detection["coordinate"]
    
    # 16ã‚¯ãƒ©ã‚¹åˆ†é¡ã‹ã‚‰æ–¹å‘ã¨å¼·åº¦ã‚’æŠ½å‡º
    direction_info = _parse_tail_direction_16class(tail_shape)
    if not direction_info:
        return _estimate_speaker_from_tails(balloon_detection, characters)
    
    direction = direction_info["direction"]  # "left", "right", "up", "down"
    intensity = direction_info["intensity"]  # "strong", "medium", "weak"
    vertical = direction_info.get("vertical", "")  # "up", "down", ""
    
    # æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨ˆç®—
    direction_vector = _calculate_direction_vector(direction, intensity, vertical)
    if not direction_vector:
        return _estimate_speaker_from_tails(balloon_detection, characters)
    
    # è©±è€…å€™è£œã‚’ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
    best_character = None
    best_score = 0
    
    for char in characters:
        if not char.get("coordinate"):
            continue
            
        char_pos = char["coordinate"]
        
        # æ–¹å‘ä¸€è‡´åº¦ã‚¹ã‚³ã‚¢
        direction_score = _calculate_direction_score(
            balloon_center, char_pos, direction_vector, intensity
        )
        
        # è·é›¢ã‚¹ã‚³ã‚¢
        distance = np.sqrt(
            (char_pos[0] - balloon_center[0])**2 + 
            (char_pos[1] - balloon_center[1])**2
        )
        distance_score = max(0, 1.0 - distance / 0.5)
        
        # ãƒ¬ã‚¤äº¤å·®ã‚¹ã‚³ã‚¢
        intersection_score = _calculate_intersection_score(
            balloon_center, direction_vector, char, char_pos
        )
        
        # ç·åˆã‚¹ã‚³ã‚¢ï¼ˆæ–¹å‘é‡è¦–ï¼‰
        total_score = (
            direction_score * 0.5 +
            distance_score * 0.2 +
            intersection_score * 0.3
        )
        
        logger.debug(f"ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ {char.get('character', 'unknown')}: "
                    f"æ–¹å‘ã‚¹ã‚³ã‚¢={direction_score:.2f}, è·é›¢ã‚¹ã‚³ã‚¢={distance_score:.2f}, "
                    f"äº¤å·®ã‚¹ã‚³ã‚¢={intersection_score:.2f}, ç·åˆ={total_score:.2f}")
        
        if total_score > best_score and total_score > 0.4:  # é–¾å€¤ä¸Šã’
            best_score = total_score
            best_character = char
    
    if best_character:
        return best_character.get("character") or best_character.get("characterId")
    
    return None


def _parse_tail_direction_16class(tail_shape: str) -> Optional[Dict[str, str]]:
    """16ã‚¯ãƒ©ã‚¹åˆ†é¡çµæœã‹ã‚‰æ–¹å‘ã¨å¼·åº¦ã‚’è§£æ"""
    
    # ä¸Šä¸‹ã¨å·¦å³ã®çµ„ã¿åˆã‚ã›ã‚’è©³ç´°ã«ãƒã‚§ãƒƒã‚¯
    if "ä¸‹å·¦" in tail_shape or ("ä¸‹" in tail_shape and "å·¦" in tail_shape):
        direction = "left"
        vertical = "down"
    elif "ä¸‹å³" in tail_shape or ("ä¸‹" in tail_shape and "å³" in tail_shape):
        direction = "right"
        vertical = "down"
    elif "ä¸Šå·¦" in tail_shape or ("ä¸Š" in tail_shape and "å·¦" in tail_shape):
        direction = "left"
        vertical = "up"
    elif "ä¸Šå³" in tail_shape or ("ä¸Š" in tail_shape and "å³" in tail_shape):
        direction = "right"
        vertical = "up"
    elif "å·¦" in tail_shape:
        direction = "left"
        vertical = ""
    elif "å³" in tail_shape:
        direction = "right"
        vertical = ""
    elif "çœŸä¸Š" in tail_shape or ("ä¸Š" in tail_shape and "ä¸‹" not in tail_shape):
        direction = "up"
        vertical = "up"
    elif "çœŸä¸‹" in tail_shape or "ä¸‹" in tail_shape:
        direction = "down"
        vertical = "down"
    else:
        return None
    
    # å¼·åº¦åˆ¤å®š
    if "30åº¦ä»¥ä¸Š" in tail_shape:
        intensity = "strong"
    elif "å°‘ã—" in tail_shape:
        intensity = "medium"
    elif "ã‚„ã‚„" in tail_shape:
        intensity = "weak"
    else:
        intensity = "medium"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    return {
        "direction": direction,
        "intensity": intensity,
        "vertical": vertical
    }


def _calculate_direction_vector(direction: str, intensity: str, vertical: str) -> Optional[List[float]]:
    """æ–¹å‘ã¨å¼·åº¦ã‹ã‚‰æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨ˆç®—"""
    
    # åŸºæœ¬è§’åº¦ï¼ˆåº¦ï¼‰
    base_angles = {
        "strong": 60,   # 30åº¦ä»¥ä¸Š
        "medium": 20,   # å°‘ã—
        "weak": 5       # ã‚„ã‚„
    }
    
    angle = base_angles.get(intensity, 20)
    
    if direction == "left":
        if vertical == "up":
            # ä¸Šå·¦æ–¹å‘
            rad = np.radians(180 - angle)
        elif vertical == "down":
            # ä¸‹å·¦æ–¹å‘
            rad = np.radians(180 + angle)
        else:
            # çœŸå·¦
            rad = np.radians(180)
    elif direction == "right":
        if vertical == "up":
            # ä¸Šå³æ–¹å‘
            rad = np.radians(angle)
        elif vertical == "down":
            # ä¸‹å³æ–¹å‘
            rad = np.radians(-angle)
        else:
            # çœŸå³
            rad = np.radians(0)
    elif direction == "up":
        rad = np.radians(90)
    elif direction == "down":
        rad = np.radians(-90)
    else:
        return None
    
    # æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆå³ã‚’0åº¦ã¨ã—ã¦åæ™‚è¨ˆå›ã‚Šï¼‰
    return [np.cos(rad), -np.sin(rad)]  # ç”»åƒåº§æ¨™ç³»ã§ã¯ä¸ŠãŒè² 


def _calculate_direction_score(balloon_center: List[float], char_pos: List[float], 
                              direction_vector: List[float], intensity: str) -> float:
    """æ–¹å‘ä¸€è‡´åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
    
    # å¹ãå‡ºã—ã‹ã‚‰ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã¸ã®ãƒ™ã‚¯ãƒˆãƒ«
    char_vector = [
        char_pos[0] - balloon_center[0],
        char_pos[1] - balloon_center[1]
    ]
    
    if char_vector[0] == 0 and char_vector[1] == 0:
        return 0
    
    # ãƒ™ã‚¯ãƒˆãƒ«ã‚’æ­£è¦åŒ–
    char_mag = np.sqrt(char_vector[0]**2 + char_vector[1]**2)
    char_normalized = [char_vector[0] / char_mag, char_vector[1] / char_mag]
    
    # å†…ç©ï¼ˆã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ï¼‰
    dot_product = (
        direction_vector[0] * char_normalized[0] + 
        direction_vector[1] * char_normalized[1]
    )
    
    # è§’åº¦è¨±å®¹ç¯„å›²ï¼ˆå¼·åº¦ã«ã‚ˆã£ã¦èª¿æ•´ï¼‰
    tolerance = {
        "strong": 0.7,   # ç´„45åº¦ã¾ã§
        "medium": 0.5,   # ç´„60åº¦ã¾ã§
        "weak": 0.3      # ç´„73åº¦ã¾ã§
    }
    
    min_similarity = tolerance.get(intensity, 0.5)
    
    # ã‚¹ã‚³ã‚¢è¨ˆç®—
    if dot_product >= min_similarity:
        return dot_product
    else:
        return max(0, dot_product * 0.5)  # éƒ¨åˆ†ç‚¹


def _calculate_intersection_score(balloon_center: List[float], direction_vector: List[float],
                                char: Dict, char_pos: List[float]) -> float:
    """ãƒ¬ã‚¤äº¤å·®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
    
    # è¤‡æ•°è·é›¢ã§ãƒ¬ã‚¤ã‚­ãƒ£ã‚¹ãƒˆ
    max_distance = 0.6  # æ­£è¦åŒ–åº§æ¨™ã§ã®æœ€å¤§è·é›¢
    steps = 50
    
    for i in range(1, steps + 1):
        t = (i / steps) * max_distance
        ray_point = [
            balloon_center[0] + direction_vector[0] * t,
            balloon_center[1] + direction_vector[1] * t
        ]
        
        # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã¨äº¤å·®ãƒã‚§ãƒƒã‚¯
        if char.get('boundingBox'):
            bbox = char['boundingBox']
            if _point_in_bbox(ray_point, bbox):
                return 1.0
        else:
            # åº§æ¨™ãƒ™ãƒ¼ã‚¹ã®è¿‘æ¥ãƒã‚§ãƒƒã‚¯
            distance_to_char = np.sqrt(
                (ray_point[0] - char_pos[0])**2 + 
                (ray_point[1] - char_pos[1])**2
            )
            if distance_to_char < 0.05:  # è¿‘æ¥åˆ¤å®š
                return 0.8
    
    return 0


def _point_in_bbox(point: List[float], bbox: Dict) -> bool:
    """ç‚¹ãŒãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹å†…ã«ã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
    return (bbox.get('x1', 0) <= point[0] <= bbox.get('x2', 1) and
            bbox.get('y1', 0) <= point[1] <= bbox.get('y2', 1))


# ã—ã£ã½ã‚¿ã‚¤ãƒ—ã®å®šç¾©
TAIL_CLASSES = {
    0: "balloon_tail",  # é€šå¸¸ã®å¹ãå‡ºã—ã®ã—ã£ã½
    1: "thought_tail",  # æ€è€ƒã®å¹ãå‡ºã—ã®ã—ã£ã½ï¼ˆç‚¹ç·šçŠ¶ï¼‰
}


class BalloonTailDetector:
    """å¹ãå‡ºã—ã®ã—ã£ã½æ¤œå‡ºå™¨"""

    def __init__(
        self,
        model_path: str = "/Users/esuji/work/fun_annotator/balloon_tail_detector/train_20250726_022600/weights/best.pt",
    ):
        """
        åˆæœŸåŒ–

        Args:
            model_path: ã—ã£ã½æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
        """
        self.model_path = model_path
        self.model = None
        self._load_model()

    def _load_model(self):
        """ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿"""
        try:
            if Path(self.model_path).exists():
                self.model = YOLO(self.model_path)
                logger.info(f"âœ… ã—ã£ã½æ¤œå‡ºãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {self.model_path}")
            else:
                logger.warning(f"âš ï¸ ã—ã£ã½æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.model_path}")
        except Exception as e:
            logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

    def detect_tails(
        self,
        balloon_image: np.ndarray,
        balloon_type: str,
        confidence_threshold: float = 0.25,
    ) -> List[Dict[str, Any]]:
        """
        å¹ãå‡ºã—ç”»åƒã‹ã‚‰ã—ã£ã½ã‚’æ¤œå‡º

        Args:
            balloon_image: å¹ãå‡ºã—éƒ¨åˆ†ã®ç”»åƒï¼ˆnumpy arrayï¼‰
            balloon_type: å¹ãå‡ºã—ã‚¿ã‚¤ãƒ—
            confidence_threshold: æ¤œå‡ºé–¾å€¤

        Returns:
            ã—ã£ã½æ¤œå‡ºçµæœã®ãƒªã‚¹ãƒˆ
        """
        if self.model is None:
            logger.warning("ã—ã£ã½æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            return []

        # ã‚ªãƒ•ã‚»ãƒªãƒ•ã¾ãŸã¯ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å°‚ç”¨å¹ãå‡ºã—ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if balloon_type == "offserif_bubble" or balloon_type.startswith(
            "chractor_bubble_"
        ):
            return []

        try:
            # YOLOæ¤œå‡ºå®Ÿè¡Œ
            results = self.model(balloon_image, conf=confidence_threshold)

            detections = []
            for r in results:
                if r.boxes is not None:
                    for i, box in enumerate(r.boxes):
                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                        conf = box.conf[0].cpu().numpy()
                        cls = int(box.cls[0].cpu().numpy())

                        # ã—ã£ã½ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
                        tail_type = TAIL_CLASSES.get(cls, "unknown")

                        # æ€è€ƒå¹ãå‡ºã—ã®å ´åˆã¯ thought_tail ã®ã¿æ¤œå‡º
                        if (
                            balloon_type == "thought_bubble"
                            and tail_type != "thought_tail"
                        ):
                            continue
                        # ãã®ä»–ã®å¹ãå‡ºã—ã®å ´åˆã¯ balloon_tail ã®ã¿æ¤œå‡º
                        elif (
                            balloon_type != "thought_bubble"
                            and tail_type != "balloon_tail"
                        ):
                            continue

                        # ç›¸å¯¾åº§æ¨™ã‚’è¨ˆç®—ï¼ˆå¹ãå‡ºã—ç”»åƒå†…ã§ã®ä½ç½®ï¼‰
                        center_x = (x1 + x2) / 2 / balloon_image.shape[1]
                        center_y = (y1 + y2) / 2 / balloon_image.shape[0]

                        detection = {
                            "tailId": f"tail_{i + 1}",
                            "type": tail_type,
                            "boundingBox": {
                                "x1": float(x1),
                                "y1": float(y1),
                                "x2": float(x2),
                                "y2": float(y2),
                                "width": float(x2 - x1),
                                "height": float(y2 - y1),
                            },
                            "relativePosition": [float(center_x), float(center_y)],
                            "confidence": float(conf),
                            "direction": self._estimate_tail_direction(
                                center_x, center_y
                            ),
                        }

                        detections.append(detection)

            return detections

        except Exception as e:
            logger.error(f"ã—ã£ã½æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def _estimate_tail_direction(self, x: float, y: float) -> str:
        """
        ã—ã£ã½ã®ä½ç½®ã‹ã‚‰æŒ‡ã—ç¤ºã™æ–¹å‘ã‚’æ¨å®š

        Args:
            x: ç›¸å¯¾Xåº§æ¨™ (0.0-1.0)
            y: ç›¸å¯¾Yåº§æ¨™ (0.0-1.0)

        Returns:
            æ–¹å‘ (top-left, top-right, bottom-left, bottom-right, center)
        """
        # å¹ãå‡ºã—ã‚’3x3ã®ã‚°ãƒªãƒƒãƒ‰ã«åˆ†å‰²ã—ã¦æ–¹å‘ã‚’åˆ¤å®š
        if y < 0.33:  # ä¸Šéƒ¨
            if x < 0.33:
                return "top-left"
            elif x > 0.67:
                return "top-right"
            else:
                return "top"
        elif y > 0.67:  # ä¸‹éƒ¨
            if x < 0.33:
                return "bottom-left"
            elif x > 0.67:
                return "bottom-right"
            else:
                return "bottom"
        else:  # ä¸­å¤®
            if x < 0.33:
                return "left"
            elif x > 0.67:
                return "right"
            else:
                return "center"


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ï¼‰
_balloon_detector = None
_tail_detector = None
_tail_shape_classifier = None


def get_balloon_detector() -> BalloonDetector:
    """å¹ãå‡ºã—æ¤œå‡ºå™¨ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _balloon_detector
    if _balloon_detector is None:
        _balloon_detector = BalloonDetector()
    return _balloon_detector


def get_tail_detector() -> BalloonTailDetector:
    """ã—ã£ã½æ¤œå‡ºå™¨ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _tail_detector
    if _tail_detector is None:
        _tail_detector = BalloonTailDetector()
    return _tail_detector


def get_tail_shape_classifier() -> TailShapeClassifier:
    """å°»å°¾å½¢çŠ¶åˆ†é¡å™¨ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _tail_shape_classifier
    if _tail_shape_classifier is None:
        _tail_shape_classifier = TailShapeClassifier()
    return _tail_shape_classifier


def extract_tail_image(image: np.ndarray, tail_bbox: Dict[str, float], padding: int = 5) -> np.ndarray:
    """
    å°»å°¾éƒ¨åˆ†ã®ç”»åƒã‚’åˆ‡ã‚Šå‡ºã—
    
    Args:
        image: å…ƒç”»åƒ
        tail_bbox: å°»å°¾ã®ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹
        padding: ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆãƒ”ã‚¯ã‚»ãƒ«ï¼‰
    
    Returns:
        åˆ‡ã‚Šå‡ºã—ãŸå°»å°¾ç”»åƒ
    """
    try:
        h, w = image.shape[:2]
        
        # åº§æ¨™ã‚’å–å¾—ï¼ˆglobalBoundingBoxã®å ´åˆã¯æ—¢ã«çµ¶å¯¾åº§æ¨™ã€boundingBoxã®å ´åˆã¯ç›¸å¯¾åº§æ¨™ï¼‰
        x1 = tail_bbox.get('x1', 0)
        y1 = tail_bbox.get('y1', 0) 
        x2 = tail_bbox.get('x2', 0)
        y2 = tail_bbox.get('y2', 0)
        
        # ç›¸å¯¾åº§æ¨™ï¼ˆ0-1ç¯„å›²ï¼‰ã®å ´åˆã¯çµ¶å¯¾åº§æ¨™ã«å¤‰æ›
        if 0 <= x1 <= 1 and 0 <= y1 <= 1 and 0 <= x2 <= 1 and 0 <= y2 <= 1:
            x1 = int(x1 * w)
            y1 = int(y1 * h)
            x2 = int(x2 * w)
            y2 = int(y2 * h)
        else:
            # æ—¢ã«çµ¶å¯¾åº§æ¨™ã®å ´åˆ
            x1 = int(x1)
            y1 = int(y1)
            x2 = int(x2)
            y2 = int(y2)
        
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¿½åŠ 
        x1 = max(0, x1 - padding)
        y1 = max(0, y1 - padding)
        x2 = min(w, x2 + padding)
        y2 = min(h, y2 + padding)
        
        # åˆ‡ã‚Šå‡ºã—
        tail_image = image[y1:y2, x1:x2]
        
        return tail_image
        
    except Exception as e:
        logger.error(f"å°»å°¾ç”»åƒåˆ‡ã‚Šå‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
        return np.array([])


def classify_tail_shapes_in_detections(
    image: np.ndarray, 
    balloon_detections: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """
    æ¤œå‡ºã•ã‚ŒãŸå¹ãå‡ºã—ã®å°»å°¾å½¢çŠ¶ã‚’åˆ†é¡
    
    Args:
        image: å…ƒç”»åƒ
        balloon_detections: å¹ãå‡ºã—æ¤œå‡ºçµæœ
    
    Returns:
        å°»å°¾å½¢çŠ¶åˆ†é¡çµæœã‚’å«ã‚€æ¤œå‡ºçµæœ
    """
    classifier = get_tail_shape_classifier()
    
    if not classifier.is_loaded:
        logger.warning("å°»å°¾å½¢çŠ¶åˆ†é¡å™¨ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return balloon_detections
    
    logger.info(f"âœ… å°»å°¾å½¢çŠ¶åˆ†é¡å™¨ãŒåˆ©ç”¨å¯èƒ½ã§ã™ ({len(balloon_detections)}å€‹ã®æ¤œå‡ºçµæœã‚’å‡¦ç†)")
    
    # æ¤œå‡ºçµæœã‚’ã‚³ãƒ”ãƒ¼
    enhanced_detections = []
    
    for detection in balloon_detections:
        enhanced_detection = detection.copy()
        
        # å°»å°¾æƒ…å ±ãŒã‚ã‚‹å ´åˆã®ã¿åˆ†é¡ï¼ˆ"tails"é…åˆ—ã‚’å‡¦ç†ï¼‰
        if 'tails' in detection and detection['tails']:
            logger.info(f"ğŸ“ å°»å°¾ã‚ã‚Šæ¤œå‡º: {detection.get('dialogueId', 'unknown')} ({len(detection['tails'])}å€‹ã®å°»å°¾)")
            
            # æœ€ã‚‚ä¿¡é ¼åº¦ãŒé«˜ã„å°»å°¾ã‚’ä½¿ç”¨
            best_tail = max(detection['tails'], key=lambda t: t.get('confidence', 0.0))
            
            try:
                # å°»å°¾ç”»åƒã‚’åˆ‡ã‚Šå‡ºã—ï¼ˆglobalBoundingBoxã¾ãŸã¯boundingBoxã‚’ä½¿ç”¨ï¼‰
                tail_bbox = best_tail.get('globalBoundingBox', best_tail.get('boundingBox', {}))
                tail_image = extract_tail_image(image, tail_bbox)
                
                if tail_image.size > 0:
                    # å°»å°¾å½¢çŠ¶ã‚’åˆ†é¡
                    classification_result = classifier.classify_tail_shape(tail_image)
                    
                    # åˆ†é¡çµæœã‚’è¿½åŠ 
                    enhanced_detection['tail_shape_classification'] = classification_result
                    
                    # åˆ†é¡çµæœã‚’tailæƒ…å ±ã«ã‚‚åæ˜ 
                    if 'tails' in enhanced_detection:
                        for tail in enhanced_detection['tails']:
                            # ä»£æ›¿æ¡ˆã®é¸æŠãƒ­ã‚¸ãƒƒã‚¯
                            category = classification_result.get('predicted_category', 'unknown')
                            confidence = classification_result.get('confidence', 0.0)
                            
                            if category in EXCLUDED_TAIL_CATEGORIES:
                                top3_predictions = classification_result.get('top3_predictions', [])
                                for prediction in top3_predictions:
                                    alt_category = prediction.get('category', 'unknown')
                                    alt_confidence = prediction.get('confidence', 0.0)
                                    if alt_category not in EXCLUDED_TAIL_CATEGORIES:
                                        category = alt_category
                                        confidence = alt_confidence
                                        break
                            
                            tail['shape_category'] = category
                            tail['shape_confidence'] = confidence
                    
                    logger.info(f"ğŸ¯ å°»å°¾å½¢çŠ¶åˆ†é¡å®Œäº†: {classification_result['predicted_category']} "
                              f"(ä¿¡é ¼åº¦: {classification_result['confidence']:.2%})")
                else:
                    logger.warning("âš ï¸ å°»å°¾ç”»åƒã®åˆ‡ã‚Šå‡ºã—ã«å¤±æ•—")
                    
            except Exception as e:
                logger.error(f"âŒ å°»å°¾å½¢çŠ¶åˆ†é¡ã‚¨ãƒ©ãƒ¼: {e}")
                import traceback
                traceback.print_exc()
        else:
            logger.debug(f"ğŸ“ å°»å°¾ãªã—æ¤œå‡º: {detection.get('dialogueId', 'unknown')}")
        
        # å½é™½æ€§ã—ã£ã½ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if 'tails' in enhanced_detection and enhanced_detection['tails']:
            filtered_tails = []
            for tail in enhanced_detection['tails']:
                tail_category = tail.get('shape_category', '')
                tail_confidence = tail.get('shape_confidence', 0.0)
                
                # ã€Œã—ã£ã½ã˜ã‚ƒãªã„ã€ã§é«˜ä¿¡é ¼åº¦ã®å ´åˆã¯é™¤å¤–
                if tail_category == "ã—ã£ã½ã˜ã‚ƒãªã„" and tail_confidence > 0.8:
                    print(f"ğŸš« å½é™½æ€§ã—ã£ã½ã‚’é™¤å¤–: {tail_category} ({tail_confidence:.1%})")
                    continue
                
                filtered_tails.append(tail)
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®ã—ã£ã½é…åˆ—ã‚’æ›´æ–°
            enhanced_detection['tails'] = filtered_tails
            
            # ã—ã£ã½ãŒã™ã¹ã¦é™¤å¤–ã•ã‚ŒãŸå ´åˆã¯tailsã‚­ãƒ¼ã‚’å‰Šé™¤
            if not filtered_tails:
                del enhanced_detection['tails']
                print(f"ğŸ“ {enhanced_detection.get('dialogueId', 'unknown')}: å…¨ã—ã£ã½ãŒå½é™½æ€§ã¨ã—ã¦é™¤å¤–ã•ã‚Œã¾ã—ãŸ")
        
        enhanced_detections.append(enhanced_detection)
    
    return enhanced_detections


def draw_tail_shape_results_on_image(
    image: np.ndarray, 
    balloon_detections: List[Dict[str, Any]],
    character_detections: Optional[List[Dict[str, Any]]] = None
) -> np.ndarray:
    """
    ç”»åƒä¸Šã«å°»å°¾å½¢çŠ¶åˆ†é¡çµæœã¨äººç‰©æ¤œå‡ºçµæœã‚’ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤æç”»
    
    Args:
        image: å…ƒç”»åƒ
        balloon_detections: å°»å°¾å½¢çŠ¶åˆ†é¡çµæœã‚’å«ã‚€æ¤œå‡ºçµæœ
        character_detections: äººç‰©æ¤œå‡ºçµæœï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    
    Returns:
        çµæœãŒæç”»ã•ã‚ŒãŸç”»åƒ
    """
    import cv2
    from PIL import Image, ImageDraw, ImageFont
    import numpy as np
    
    # OpenCVç”»åƒã‚’PILç”»åƒã«å¤‰æ›
    if len(image.shape) == 2:
        pil_image = Image.fromarray(image)
    else:
        pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    
    draw = ImageDraw.Draw(pil_image)
    
    # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®šï¼ˆå…¨ä½“çš„ã«å°ã•ãã™ã‚‹ï¼‰
    try:
        # macOSã®æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆ
        font = ImageFont.truetype("/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒè§’ã‚´ã‚·ãƒƒã‚¯ W3.ttc", 16)  # 24â†’16
        font_small = ImageFont.truetype("/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒè§’ã‚´ã‚·ãƒƒã‚¯ W3.ttc", 12)  # 20â†’12
    except:
        try:
            # Linuxã®æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆ
            font = ImageFont.truetype("/usr/share/fonts/truetype/fonts-japanese-gothic.ttf", 16)
            font_small = ImageFont.truetype("/usr/share/fonts/truetype/fonts-japanese-gothic.ttf", 12)
        except:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            font = ImageFont.load_default()
            font_small = ImageFont.load_default()
    
    # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è‰²å®šç¾©ï¼ˆã‚ˆã‚Šæ¿ƒã„è‰²åˆã„ï¼‰
    CHARACTER_COLORS = {
        'é‡ã€…åŸã‚†ãšã“': (255, 105, 180),  # yuzuko - ãƒ›ãƒƒãƒˆãƒ”ãƒ³ã‚¯ï¼ˆã‚ˆã‚Šæ¿ƒãï¼‰
        'æ—¥å‘ç¸': (147, 112, 219),        # yukari - ãƒŸãƒ‡ã‚£ã‚¢ãƒ ãƒ‘ãƒ¼ãƒ—ãƒ«ï¼ˆã‚ˆã‚Šæ¿ƒãï¼‰
        'æ«Ÿäº•å”¯': (255, 215, 0),          # yui - ã‚´ãƒ¼ãƒ«ãƒ‰ï¼ˆã‚ˆã‚Šæ¿ƒãï¼‰
        'æ¾æœ¬é ¼å­': (255, 140, 105),      # yoriko - ã‚µãƒ¼ãƒ¢ãƒ³ï¼ˆã‚ˆã‚Šæ¿ƒãï¼‰
        'ç›¸å·åƒç©‚': (152, 251, 152),      # chiho - ãƒšãƒ¼ãƒ«ã‚°ãƒªãƒ¼ãƒ³ï¼ˆã‚ˆã‚Šæ¿ƒãï¼‰
        'å²¡é‡ä½³': (222, 184, 135),        # kei - ãƒãƒ¼ãƒªãƒ¼ã‚¦ãƒƒãƒ‰ï¼ˆã‚ˆã‚Šæ¿ƒãï¼‰
        'é•·è°·å·ãµã¿': (211, 211, 211),    # fumi - ãƒ©ã‚¤ãƒˆã‚°ãƒ¬ãƒ¼ï¼ˆã‚ˆã‚Šæ¿ƒãï¼‰
        'ãŠæ¯ã•ã‚“': (255, 182, 193),      # ãƒ©ã‚¤ãƒˆãƒ”ãƒ³ã‚¯ï¼ˆã‚ˆã‚Šæ¿ƒãï¼‰
        'å…ˆç”Ÿ': (173, 216, 230),          # ãƒ©ã‚¤ãƒˆãƒ–ãƒ«ãƒ¼ï¼ˆã‚ˆã‚Šæ¿ƒãï¼‰
        'ç”Ÿå¾’A': (221, 160, 221),        # ãƒ—ãƒ©ãƒ ï¼ˆã‚ˆã‚Šæ¿ƒãï¼‰
        'ç”Ÿå¾’B': (175, 238, 238),        # ãƒ‘ãƒ¼ãƒ«ã‚¿ãƒ¼ã‚³ã‚¤ã‚ºï¼ˆã‚ˆã‚Šæ¿ƒãï¼‰
        'ãã®ä»–': (245, 222, 179)         # ã‚¦ã‚£ãƒ¼ãƒˆï¼ˆã‚ˆã‚Šæ¿ƒãï¼‰
    }
    
    # è‹±èªåã‹ã‚‰æ—¥æœ¬èªåã¸ã®å¤‰æ›
    ENGLISH_TO_JAPANESE = {
        'yuzuko': 'é‡ã€…åŸã‚†ãšã“',
        'yukari': 'æ—¥å‘ç¸',
        'yui': 'æ«Ÿäº•å”¯',
        'yoriko': 'æ¾æœ¬é ¼å­',
        'chiho': 'ç›¸å·åƒç©‚',
        'kei': 'å²¡é‡ä½³',
        'fumi': 'é•·è°·å·ãµã¿',
        'unknown': 'ä¸æ˜'
    }
    
    # äººç‰©æ¤œå‡ºçµæœã‚’æç”»
    if character_detections:
        for char in character_detections:
            bbox = char.get('boundingBox', {})
            x1 = int(bbox.get('x1', 0))
            y1 = int(bbox.get('y1', 0))
            x2 = int(bbox.get('x2', 100))
            y2 = int(bbox.get('y2', 100))
            
            # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åã‚’å–å¾—ã—ã¦æ—¥æœ¬èªåã«å¤‰æ›
            char_name = char.get('characterName', 'Unknown')
            japanese_name = ENGLISH_TO_JAPANESE.get(char_name.lower(), char_name)
            
            # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æ¯ã®è‰²ã‚’å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯é’è‰²ï¼‰
            char_color = CHARACTER_COLORS.get(japanese_name, (0, 0, 255))
            
            # äººç‰©ã®æ ã‚’æç”»ï¼ˆã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æ¯ã®è‰²ï¼‰
            draw.rectangle([(x1, y1), (x2, y2)], outline=char_color, width=3)
            
            # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åã¨ä¿¡é ¼åº¦ã‚’è¡¨ç¤º
            confidence = char.get('confidence', 0.0)
            label = f"{japanese_name} ({confidence:.1%})"
            
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’å†…å´ä¸‹éƒ¨ã«é…ç½®ï¼ˆå·¦ä¸‹è§’ã‹ã‚‰5pxå†…å´ï¼‰
            text_x = x1 + 5
            text_y = y2 - 25  # ä¸‹ã‹ã‚‰25pxä¸Šã«é…ç½®
            
            # ãƒ†ã‚­ã‚¹ãƒˆã®èƒŒæ™¯ã‚’æç”»ï¼ˆãƒãƒ¼ã‚¸ãƒ§ãƒ³äº’æ›æ€§å¯¾å¿œï¼‰
            try:
                # æ–°ã—ã„Pillowãƒãƒ¼ã‚¸ãƒ§ãƒ³
                text_bbox = draw.textbbox((text_x, text_y), label, font=font_small)
                draw.rectangle(text_bbox, fill=char_color)
            except AttributeError:
                # å¤ã„Pillowãƒãƒ¼ã‚¸ãƒ§ãƒ³
                text_size = draw.textsize(label, font=font_small)
                draw.rectangle((text_x, text_y, text_x + text_size[0], text_y + text_size[1]), fill=char_color)
            
            draw.text((text_x, text_y), label, font=font_small, fill=(0, 0, 0))  # é»’æ–‡å­—
    
    # å¹ãå‡ºã—æ¤œå‡ºçµæœã‚’æç”»
    for detection in balloon_detections:
        # å¹ãå‡ºã—ã®ä½ç½®ã‚’å–å¾—
        bbox = detection.get('boundingBox', {})
        x1 = int(bbox.get('x1', 0))
        y1 = int(bbox.get('y1', 0))
        x2 = int(bbox.get('x2', 100))
        y2 = int(bbox.get('y2', 100))
        
        # å¹ãå‡ºã—ã®æ ã‚’æç”»ï¼ˆã‚ªãƒ¬ãƒ³ã‚¸è‰²ï¼‰
        draw.rectangle([(x1, y1), (x2, y2)], outline=(255, 165, 0), width=2)
        
        # å¹ãå‡ºã—ã‚¿ã‚¤ãƒ—ã‚’å†…å´ä¸Šéƒ¨ã«è¡¨ç¤ºï¼ˆã‚ˆã‚Šå°ã•ãªãƒ•ã‚©ãƒ³ãƒˆï¼‰
        balloon_type = detection.get('balloon_type', detection.get('type', 'unknown'))
        if balloon_type and balloon_type != 'unknown':
            # å¹ãå‡ºã—ã‚¿ã‚¤ãƒ—ã®ãƒ©ãƒ™ãƒ«ã‚’ä½œæˆï¼ˆç•¥èªåŒ–ï¼‰
            type_labels = {
                'speech_bubble': 'Speech',
                'thought_bubble': 'Thought', 
                'exclamation_bubble': 'Exclamation',
                'scream_bubble': 'Scream',
                'whisper_bubble': 'Whisper',
                'narration': 'Narration',
                'onomatopoeia': 'Onoma'
            }
            type_label = type_labels.get(balloon_type, balloon_type.replace('_', ' ').title())
            
            # å†…å´ä¸Šéƒ¨ã®ä½ç½®ã‚’è¨ˆç®—ï¼ˆå·¦ä¸Šè§’ã‹ã‚‰3pxå†…å´ï¼‰
            type_x = x1 + 3
            type_y = y1 + 3
            
            # ã•ã‚‰ã«å°ã•ã„ãƒ•ã‚©ãƒ³ãƒˆã‚’ä½œæˆ
            try:
                font_tiny = ImageFont.truetype("/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒè§’ã‚´ã‚·ãƒƒã‚¯ W3.ttc", 10)  # ã‚ˆã‚Šå°ã•ã
            except:
                try:
                    font_tiny = ImageFont.truetype("/usr/share/fonts/truetype/fonts-japanese-gothic.ttf", 10)
                except:
                    font_tiny = font_small
            
            # ãƒ†ã‚­ã‚¹ãƒˆã®èƒŒæ™¯ã‚’æç”»ï¼ˆãƒãƒ¼ã‚¸ãƒ§ãƒ³äº’æ›æ€§å¯¾å¿œï¼‰
            try:
                # æ–°ã—ã„Pillowãƒãƒ¼ã‚¸ãƒ§ãƒ³
                type_text_bbox = draw.textbbox((type_x, type_y), type_label, font=font_tiny)
                draw.rectangle(type_text_bbox, fill=(255, 255, 0, 200))  # é»„è‰²èƒŒæ™¯
            except AttributeError:
                # å¤ã„Pillowãƒãƒ¼ã‚¸ãƒ§ãƒ³
                type_text_size = draw.textsize(type_label, font=font_tiny)
                draw.rectangle((type_x, type_y, type_x + type_text_size[0], type_y + type_text_size[1]), fill=(255, 255, 0, 200))
            
            # ã‚¿ã‚¤ãƒ—ãƒ†ã‚­ã‚¹ãƒˆã‚’æç”»ï¼ˆé»’è‰²ï¼‰
            draw.text((type_x, type_y), type_label, font=font_tiny, fill=(0, 0, 0))
        
        # å°»å°¾å½¢çŠ¶åˆ†é¡çµæœã®è¡¨ç¤ºã¯å‰Šé™¤ï¼ˆç·‘åœ°ã«é»’æ–‡å­—ã®è¡¨ç¤ºã‚’ãªãã™ï¼‰
            
            # ã—ã£ã½é ˜åŸŸã‚’æç”»
            print(f"ğŸ¨ å¯è¦–åŒ–: {balloon_type} - ã—ã£ã½ã‚ã‚Š: {'tails' in detection}")
            if 'tails' in detection:
                print(f"ğŸ¨ ã—ã£ã½æ•°: {len(detection['tails'])}")
                for tail in detection['tails']:
                    # ã—ã£ã½ã®ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’å–å¾—ï¼ˆçµ¶å¯¾åº§æ¨™ï¼‰
                    tail_bbox = tail.get('globalBoundingBox', tail.get('boundingBox', {}))
                    if tail_bbox:
                        tx1 = int(tail_bbox.get('x1', 0))
                        ty1 = int(tail_bbox.get('y1', 0))
                        tx2 = int(tail_bbox.get('x2', 0))
                        ty2 = int(tail_bbox.get('y2', 0))
                        
                        # ç›¸å¯¾åº§æ¨™ï¼ˆ0-1ç¯„å›²ï¼‰ã®å ´åˆã¯çµ¶å¯¾åº§æ¨™ã«å¤‰æ›
                        if max(tx1, ty1, tx2, ty2) <= 1.0:
                            tx1 = int(tx1 * img_width)
                            ty1 = int(ty1 * img_height)
                            tx2 = int(tx2 * img_width)
                            ty2 = int(ty2 * img_height)
                        
                        # ã—ã£ã½ã®çŸ©å½¢ã‚’æç”»ï¼ˆç´«è‰²ã€åŠé€æ˜ï¼‰
                        draw.rectangle(
                            [tx1, ty1, tx2, ty2],
                            outline=(128, 0, 128),
                            width=2
                        )
                        
                        # ã—ã£ã½ã®ä¸­å¿ƒç‚¹ã‚’æç”»
                        center_x = (tx1 + tx2) // 2
                        center_y = (ty1 + ty2) // 2
                        draw.ellipse(
                            [center_x - 3, center_y - 3, center_x + 3, center_y + 3],
                            fill=(128, 0, 128)
                        )
                        
                        # ã—ã£ã½ã®æ–¹å‘ãƒ©ãƒ™ãƒ«ã‚’æç”»ï¼ˆä¿¡é ¼åº¦ä»˜ãï¼‰
                        if 'shape_category' in tail and tail['shape_category']:
                            tail_label = tail['shape_category']
                            # ä¿¡é ¼åº¦ã‚’è¿½åŠ ï¼ˆãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆè¡¨ç¤ºï¼‰
                            if 'shape_confidence' in tail and tail['shape_confidence'] is not None:
                                confidence_percent = int(tail['shape_confidence'] * 100)
                                tail_label = f"{tail_label} ({confidence_percent}%)"
                        elif 'direction' in tail:
                            tail_label = f"â†’{tail['direction']}"
                        else:
                            tail_label = "tail"
                        
                        # ãƒ†ã‚­ã‚¹ãƒˆã®èƒŒæ™¯ã‚’æç”»ï¼ˆãƒãƒ¼ã‚¸ãƒ§ãƒ³äº’æ›æ€§å¯¾å¿œï¼‰
                        try:
                            # æ–°ã—ã„Pillowãƒãƒ¼ã‚¸ãƒ§ãƒ³
                            text_bbox = draw.textbbox((tx1, ty1 - 20), tail_label, font=font_small)
                            draw.rectangle(text_bbox, fill=(128, 0, 128, 180))
                        except AttributeError:
                            # å¤ã„Pillowãƒãƒ¼ã‚¸ãƒ§ãƒ³
                            text_size = draw.textsize(tail_label, font=font_small)
                            draw.rectangle((tx1, ty1 - 20, tx1 + text_size[0], ty1 - 20 + text_size[1]), fill=(128, 0, 128, 180))
                        
                        draw.text((tx1, ty1 - 20), tail_label, font=font_small, fill=(255, 255, 255))
    
    # PILç”»åƒã‚’OpenCVç”»åƒã«å¤‰æ›
    result_array = np.array(pil_image)
    if len(result_array.shape) == 3:
        result_image = cv2.cvtColor(result_array, cv2.COLOR_RGB2BGR)
    else:
        result_image = result_array
    
    return result_image
