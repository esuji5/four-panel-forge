#!/usr/bin/env python3
"""
LLM Utils - ÂÆåÂÖ®ÁâàÔºàmain.py„Å®„ÅÆ‰∫íÊèõÊÄß„Çí‰øùÊåÅÔºâ

4„Ç≥„ÉûÊº´ÁîªËß£Êûê„ÅÆ„Åü„ÇÅ„ÅÆLLMÁµ±‰∏Ä„Ç§„É≥„Çø„Éº„Éï„Çß„Éº„Çπ
Êó¢Â≠ò„ÅÆllm_utils.py„Å®„ÅÆÂÆåÂÖ®„Å™ÂæåÊñπ‰∫íÊèõÊÄß„ÇíÊèê‰æõ
"""

import base64
import json
import os
import re
import shutil
import subprocess
import sys
from abc import ABC, abstractmethod
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

import anthropic
import google.genai as genai
import requests
from dotenv import load_dotenv
from google.genai.types import Part

# Áí∞Â¢ÉÂ§âÊï∞Ë™≠„ÅøËæº„Åø
load_dotenv()


# „Çø„Ç§„É†„Çπ„Çø„É≥„Éó‰ªò„Åç„É≠„Ç∞Èñ¢Êï∞
def log_with_time(message: str, level: str = "INFO"):
    """„Çø„Ç§„É†„Çπ„Çø„É≥„Éó‰ªò„Åç„Åß„É≠„Ç∞„ÇíÂá∫Âäõ"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
    prefix = {
        "INFO": "‚ÑπÔ∏è",
        "DEBUG": "üîç",
        "ERROR": "‚ùå",
        "WARNING": "‚ö†Ô∏è",
        "SUCCESS": "‚úÖ",
    }.get(level, "üìù")

    log_message = f"[{timestamp}] {prefix} {message}"
    print(log_message)
    sys.stdout.flush()  # Âç≥Â∫ß„Å´Âá∫Âäõ„ÇíÂèçÊò†


def load_config() -> Dict[str, Any]:
    """Ë®≠ÂÆö„Éï„Ç°„Ç§„É´„ÇíË™≠„ÅøËæº„ÅøÔºàÁí∞Â¢ÉÂà•Ôºâ"""
    env = os.getenv("ENVIRONMENT", "local")
    config_dir = Path("config")

    # „Éá„Éï„Ç©„É´„ÉàË®≠ÂÆö„ÇíË™≠„ÅøËæº„Åø
    default_config = {}
    default_file = config_dir / "default.json"
    if default_file.exists():
        with open(default_file, "r", encoding="utf-8") as f:
            default_config = json.load(f)

    # Áí∞Â¢ÉÂà•Ë®≠ÂÆö„ÇíË™≠„ÅøËæº„Åø
    env_config = {}
    env_file = config_dir / f"{env}.json"
    if env_file.exists():
        with open(env_file, "r", encoding="utf-8") as f:
            env_config = json.load(f)

    # Ë®≠ÂÆö„Çí„Éû„Éº„Ç∏ÔºàÁí∞Â¢ÉË®≠ÂÆö„ÅåÂÑ™ÂÖàÔºâ
    def merge_dict(base: dict, override: dict) -> dict:
        result = base.copy()
        for key, value in override.items():
            if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                result[key] = merge_dict(result[key], value)
            else:
                result[key] = value
        return result

    return merge_dict(default_config, env_config)


# Ê≠£Ë¶èË°®Áèæ„Éë„Çø„Éº„É≥
RE_PAGE_KOMA_NUM = re.compile(r"\b\d{3}-\d\b")


@dataclass
class LLMConfig:
    """LLMË®≠ÂÆö„ÇØ„É©„ÇπÔºàÁí∞Â¢ÉÂà•Ë®≠ÂÆöÂØæÂøúÔºâ"""

    openai_api_key: str = os.getenv("OPENAI_API_KEY", "")
    anthropic_api_key: str = os.getenv("ANTHROPIC_API_KEY", "")
    google_api_key: str = os.getenv("GOOGLE_API_KEY", "")

    # „Éá„Éï„Ç©„É´„Éà„É¢„Éá„É´ÔºàË®≠ÂÆö„Éï„Ç°„Ç§„É´„Åß‰∏äÊõ∏„ÅçÂèØËÉΩÔºâ
    openai_model: str = "gpt-4o-2024-11-20"
    anthropic_model: str = "claude-3-5-sonnet-20240620"
    gemini_model: str = "gemini-2.5-flash"

    # „Åù„ÅÆ‰ªñË®≠ÂÆö
    max_tokens: int = 4048
    confidence_threshold: float = 0.5
    temp_dir: str = "tmp/api_queries"

    def __post_init__(self):
        """Ë®≠ÂÆö„Éï„Ç°„Ç§„É´„Åã„ÇâÂÄ§„ÇíË™≠„ÅøËæº„Åø"""
        config = load_config()

        # LLMË®≠ÂÆö„ÇíË™≠„ÅøËæº„Åø
        llm_config = config.get("llm", {})

        # GeminiË®≠ÂÆö„ÅÆ‰∏äÊõ∏„Åç
        gemini_config = llm_config.get("gemini", {})
        if "model" in gemini_config:
            self.gemini_model = gemini_config["model"]

        # OpenAIË®≠ÂÆö„ÅÆ‰∏äÊõ∏„Åç
        openai_config = llm_config.get("openai", {})
        if "model" in openai_config:
            self.openai_model = openai_config["model"]
        if "max_tokens" in openai_config:
            self.max_tokens = openai_config["max_tokens"]

        # AnthropicË®≠ÂÆö„ÅÆ‰∏äÊõ∏„Åç
        anthropic_config = llm_config.get("anthropic", {})
        if "model" in anthropic_config:
            self.anthropic_model = anthropic_config["model"]

        # „Ç¢„Éó„É™Ë®≠ÂÆö„ÅÆ‰∏äÊõ∏„Åç
        app_config = config.get("app", {})
        if "temp_dir" in app_config:
            self.temp_dir = app_config["temp_dir"]


@dataclass
class LLMResponse:
    """LLM„É¨„Çπ„Éù„É≥„ÇπÁµ±‰∏Ä„ÇØ„É©„Çπ"""

    content: str
    model: str
    provider: str
    timestamp: datetime
    metadata: Dict[str, Any]
    raw_response: Any


class ImageProcessor:
    """ÁîªÂÉèÂá¶ÁêÜ„É¶„Éº„ÉÜ„Ç£„É™„ÉÜ„Ç£"""

    @staticmethod
    def encode_image_to_base64(image_path: Union[str, Path]) -> str:
        """ÁîªÂÉè„ÇíBase64„Ç®„É≥„Ç≥„Éº„Éâ"""
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode("utf-8")

    @staticmethod
    def clean_base64_data(base64_data: str) -> str:
        """Base64„Éá„Éº„Çø„ÅÆ„Éó„É¨„Éï„Ç£„ÉÉ„ÇØ„Çπ„ÇíÈô§Âéª"""
        if base64_data.startswith("data:image"):
            return base64_data.split(",")[1]
        return base64_data

    @staticmethod
    def save_base64_image(base64_data: str, output_path: Union[str, Path]) -> None:
        """Base64„Éá„Éº„Çø„ÇíÁîªÂÉè„Éï„Ç°„Ç§„É´„Å®„Åó„Å¶‰øùÂ≠ò"""
        cleaned_data = ImageProcessor.clean_base64_data(base64_data)
        image_bytes = base64.b64decode(cleaned_data)
        with open(output_path, "wb") as f:
            f.write(image_bytes)


class QueryLogger:
    """API„ÇØ„Ç®„É™„ÅÆ„É≠„Ç∞Ê©üËÉΩ"""

    def __init__(self, config: LLMConfig):
        self.config = config

    def create_query_dir(self, provider: str, suffix: str = "") -> Path:
        """„ÇØ„Ç®„É™‰øùÂ≠òÁî®„Éá„Ç£„É¨„ÇØ„Éà„É™‰ΩúÊàê"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
        query_dir = Path(self.config.temp_dir) / f"{provider}_{suffix}_{timestamp}"
        query_dir.mkdir(parents=True, exist_ok=True)
        return query_dir

    def save_query_info(self, query_dir: Path, query_info: Dict[str, Any]) -> None:
        """„ÇØ„Ç®„É™ÊÉÖÂ†±„Çí‰øùÂ≠ò"""
        with open(query_dir / "query_info.json", "w", encoding="utf-8") as f:
            json.dump(query_info, f, ensure_ascii=False, indent=2)

    def save_prompt(self, query_dir: Path, prompt: str) -> None:
        """„Éó„É≠„É≥„Éó„Éà„Çí‰øùÂ≠ò"""
        with open(query_dir / "prompt.txt", "w", encoding="utf-8") as f:
            f.write(prompt)


class BaseLLMProvider(ABC):
    """LLM„Éó„É≠„Éê„Ç§„ÉÄ„Éº„ÅÆÂü∫Â∫ï„ÇØ„É©„Çπ"""

    def __init__(self, config: LLMConfig):
        self.config = config
        self.logger = QueryLogger(config)

    @abstractmethod
    def generate_response(self, prompt: str, **kwargs) -> LLMResponse:
        """„É¨„Çπ„Éù„É≥„ÇπÁîüÊàê„ÅÆÊäΩË±°„É°„ÇΩ„ÉÉ„Éâ"""
        pass

    @abstractmethod
    def generate_image_response(self, prompt: str, image_path: str, **kwargs) -> LLMResponse:
        """ÁîªÂÉè‰ªò„Åç„É¨„Çπ„Éù„É≥„ÇπÁîüÊàê„ÅÆÊäΩË±°„É°„ÇΩ„ÉÉ„Éâ"""
        pass


class OpenAIProvider(BaseLLMProvider):
    """OpenAI API„Éó„É≠„Éê„Ç§„ÉÄ„Éº"""

    def __init__(self, config: LLMConfig):
        super().__init__(config)
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {config.openai_api_key}",
        }

    def generate_response(self, prompt: str, **kwargs) -> LLMResponse:
        """„ÉÜ„Ç≠„Çπ„Éà„É¨„Çπ„Éù„É≥„ÇπÁîüÊàê"""
        model = kwargs.get("model", self.config.openai_model)
        max_tokens = kwargs.get("max_tokens", self.config.max_tokens)

        payload = {
            "model": model,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": max_tokens,
        }

        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=self.headers,
            json=payload,
        )
        response.raise_for_status()

        result = response.json()
        content = result["choices"][0]["message"]["content"]

        return LLMResponse(
            content=content,
            model=model,
            provider="openai",
            timestamp=datetime.now(),
            metadata={"tokens_used": result.get("usage", {})},
            raw_response=result,
        )

    def generate_image_response(self, prompt: str, image_path: str, **kwargs) -> LLMResponse:
        """ÁîªÂÉè‰ªò„Åç„É¨„Çπ„Éù„É≥„ÇπÁîüÊàê"""
        model = kwargs.get("model", self.config.openai_model)
        max_tokens = kwargs.get("max_tokens", self.config.max_tokens)

        # ÁîªÂÉè„ÇíBase64„Ç®„É≥„Ç≥„Éº„Éâ
        base64_image = ImageProcessor.encode_image_to_base64(image_path)

        payload = {
            "model": model,
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"},
                        },
                    ],
                }
            ],
            "max_tokens": max_tokens,
        }

        # „ÇØ„Ç®„É™„É≠„Ç∞‰øùÂ≠ò
        query_dir = self.logger.create_query_dir("openai", "image")
        self.logger.save_prompt(query_dir, prompt)
        shutil.copy2(image_path, query_dir / f"image{Path(image_path).suffix}")

        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=self.headers,
            json=payload,
        )
        response.raise_for_status()

        result = response.json()
        content = result["choices"][0]["message"]["content"]

        return LLMResponse(
            content=content,
            model=model,
            provider="openai",
            timestamp=datetime.now(),
            metadata={
                "tokens_used": result.get("usage", {}),
                "query_dir": str(query_dir),
            },
            raw_response=result,
        )


class AnthropicProvider(BaseLLMProvider):
    """Anthropic Claude API„Éó„É≠„Éê„Ç§„ÉÄ„Éº"""

    def __init__(self, config: LLMConfig):
        super().__init__(config)
        self.client = anthropic.Anthropic(api_key=config.anthropic_api_key)

    def generate_response(self, prompt: str, **kwargs) -> LLMResponse:
        """„ÉÜ„Ç≠„Çπ„Éà„É¨„Çπ„Éù„É≥„ÇπÁîüÊàê"""
        model = kwargs.get("model", self.config.anthropic_model)
        max_tokens = kwargs.get("max_tokens", self.config.max_tokens)

        response = self.client.messages.create(
            model=model,
            max_tokens=max_tokens,
            messages=[{"role": "user", "content": prompt}],
        )

        content = response.content[0].text

        return LLMResponse(
            content=content,
            model=model,
            provider="anthropic",
            timestamp=datetime.now(),
            metadata={
                "usage": {
                    "input_tokens": response.usage.input_tokens,
                    "output_tokens": response.usage.output_tokens,
                }
            },
            raw_response=response,
        )

    def generate_image_response(self, prompt: str, image_path: str, **kwargs) -> LLMResponse:
        """ÁîªÂÉè‰ªò„Åç„É¨„Çπ„Éù„É≥„ÇπÁîüÊàê"""
        model = kwargs.get("model", self.config.anthropic_model)
        max_tokens = kwargs.get("max_tokens", self.config.max_tokens)

        # ÁîªÂÉè„ÇíBase64„Ç®„É≥„Ç≥„Éº„Éâ
        base64_image = ImageProcessor.encode_image_to_base64(image_path)

        # „ÇØ„Ç®„É™„É≠„Ç∞‰øùÂ≠ò
        query_dir = self.logger.create_query_dir("anthropic", "image")
        self.logger.save_prompt(query_dir, prompt)
        shutil.copy2(image_path, query_dir / f"image{Path(image_path).suffix}")

        message_content = [
            {"type": "text", "text": prompt},
            {
                "type": "image",
                "source": {
                    "type": "base64",
                    "media_type": "image/jpeg",
                    "data": base64_image,
                },
            },
        ]

        response = self.client.messages.create(
            model=model,
            max_tokens=max_tokens,
            messages=[{"role": "user", "content": message_content}],
        )

        content = response.content[0].text

        return LLMResponse(
            content=content,
            model=model,
            provider="anthropic",
            timestamp=datetime.now(),
            metadata={
                "usage": {
                    "input_tokens": response.usage.input_tokens,
                    "output_tokens": response.usage.output_tokens,
                },
                "query_dir": str(query_dir),
            },
            raw_response=response,
        )

    def generate_4koma_response(
        self, prompt: str, image_paths: List[str], image_data: Dict, **kwargs
    ) -> LLMResponse:
        """4„Ç≥„ÉûÂÖ®‰ΩìËß£Êûê"""
        model = kwargs.get("model", self.config.anthropic_model)
        max_tokens = kwargs.get("max_tokens", self.config.max_tokens)

        # „É°„ÉÉ„Çª„Éº„Ç∏„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÇíÊßãÁØâ
        message_content = []

        # ÂêÑÁîªÂÉè„ÇíËøΩÂä†
        for i, image_path in enumerate(image_paths):
            message_content.append({"type": "text", "text": f"Image {i + 1}:"})
            base64_image = ImageProcessor.encode_image_to_base64("public" + image_path)
            message_content.append(
                {
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": "image/jpeg",
                        "data": base64_image,
                    },
                }
            )

        # ÁîªÂÉè„Éá„Éº„Çø„Å®„Éó„É≠„É≥„Éó„Éà„ÇíËøΩÂä†
        message_content.append({"type": "text", "text": json.dumps(image_data)})
        message_content.append({"type": "text", "text": prompt})

        # „ÇØ„Ç®„É™„É≠„Ç∞‰øùÂ≠ò
        query_dir = self.logger.create_query_dir("anthropic", "4koma")
        self.logger.save_prompt(query_dir, prompt)

        response = self.client.messages.create(
            model=model,
            max_tokens=max_tokens,
            messages=[{"role": "user", "content": message_content}],
        )

        content = response.content[0].text

        return LLMResponse(
            content=content,
            model=model,
            provider="anthropic",
            timestamp=datetime.now(),
            metadata={
                "usage": {
                    "input_tokens": response.usage.input_tokens,
                    "output_tokens": response.usage.output_tokens,
                },
                "query_dir": str(query_dir),
            },
            raw_response=response,
        )


class GeminiProvider(BaseLLMProvider):
    """Google Gemini API„Éó„É≠„Éê„Ç§„ÉÄ„Éº"""

    def __init__(self, config: LLMConfig):
        super().__init__(config)
        self.client = genai.Client(api_key=config.google_api_key)

    def generate_response(self, prompt: str, **kwargs) -> LLMResponse:
        """„ÉÜ„Ç≠„Çπ„Éà„É¨„Çπ„Éù„É≥„ÇπÁîüÊàê"""
        model = kwargs.get("model", self.config.gemini_model)

        response = self.client.models.generate_content(model=model, contents=prompt)

        return LLMResponse(
            content=response.text,
            model=model,
            provider="gemini",
            timestamp=datetime.now(),
            metadata={"candidates": len(response.candidates)},
            raw_response=response,
        )

    def generate_image_response(self, prompt: str, image_path: str, **kwargs) -> LLMResponse:
        """ÁîªÂÉè‰ªò„Åç„É¨„Çπ„Éù„É≥„ÇπÁîüÊàê"""
        model = kwargs.get("model", self.config.gemini_model)

        log_with_time("üåê Using Gemini API", level="INFO")
        # ÁîªÂÉè„ÇíBase64„Ç®„É≥„Ç≥„Éº„Éâ
        base64_image = ImageProcessor.encode_image_to_base64(image_path)

        # „ÇØ„Ç®„É™„É≠„Ç∞‰øùÂ≠ò
        query_dir = self.logger.create_query_dir("gemini", "image")
        self.logger.save_prompt(query_dir, prompt)
        shutil.copy2(image_path, query_dir / f"image{Path(image_path).suffix}")

        # Part„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„ÅßÁîªÂÉè‰ªò„Åç„Ç≥„É≥„ÉÜ„É≥„ÉÑ„Çí‰ΩúÊàê
        contents = [
            Part(text=prompt),
            Part(inline_data={"mime_type": "image/jpeg", "data": base64_image}),
        ]

        response = self.client.models.generate_content(model=model, contents=contents)

        return LLMResponse(
            content=response.text,
            model=model,
            provider="gemini",
            timestamp=datetime.now(),
            metadata={
                "candidates": len(response.candidates),
                "query_dir": str(query_dir),
            },
            raw_response=response,
        )

    def generate_multimodal_response(self, messages: List[Dict], **kwargs) -> LLMResponse:
        """„Éû„É´„ÉÅ„É¢„Éº„ÉÄ„É´„É°„ÉÉ„Çª„Éº„Ç∏Âá¶ÁêÜ"""
        model = kwargs.get("model", self.config.gemini_model)

        # „ÇØ„Ç®„É™„É≠„Ç∞‰øùÂ≠ò
        query_dir = self.logger.create_query_dir("gemini", "multimodal")

        # „É°„ÉÉ„Çª„Éº„Ç∏„ÇíPart„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„Å´Â§âÊèõ
        content_parts = []
        for i, message in enumerate(messages):
            if message.get("type") == "text":
                content_parts.append(Part(text=message["text"]))

                # „ÉÜ„Ç≠„Çπ„Éà„Çí„Éï„Ç°„Ç§„É´„Å´‰øùÂ≠ò
                with open(query_dir / f"message_{i}_text.txt", "w", encoding="utf-8") as f:
                    f.write(message["text"])

            elif message.get("type") == "image_url":
                image_url = message["image_url"]["url"]
                if image_url.startswith("data:image"):
                    base64_data = image_url.split(",")[1]
                    content_parts.append(
                        Part(inline_data={"mime_type": "image/jpeg", "data": base64_data})
                    )

                    # ÁîªÂÉè„Çí„Éï„Ç°„Ç§„É´„Å´‰øùÂ≠ò
                    ImageProcessor.save_base64_image(
                        base64_data, query_dir / f"message_{i}_image.jpg"
                    )

        response = self.client.models.generate_content(model=model, contents=content_parts)

        return LLMResponse(
            content=response.text,
            model=model,
            provider="gemini",
            timestamp=datetime.now(),
            metadata={
                "candidates": len(response.candidates),
                "query_dir": str(query_dir),
            },
            raw_response=response,
        )


class LLMManager:
    """LLM„Éó„É≠„Éê„Ç§„ÉÄ„ÉºÁµ±‰∏ÄÁÆ°ÁêÜ„ÇØ„É©„Çπ"""

    def __init__(self, config: Optional[LLMConfig] = None):
        self.config = config or LLMConfig()
        self.providers = {
            "openai": OpenAIProvider(self.config),
            "anthropic": AnthropicProvider(self.config),
            "gemini": GeminiProvider(self.config),
        }

    def get_provider(self, provider_name: str) -> BaseLLMProvider:
        """„Éó„É≠„Éê„Ç§„ÉÄ„ÉºÂèñÂæó"""
        if provider_name not in self.providers:
            raise ValueError(f"Unsupported provider: {provider_name}")
        return self.providers[provider_name]

    def generate_response(self, provider: str, prompt: str, **kwargs) -> LLMResponse:
        """Áµ±‰∏Ä„É¨„Çπ„Éù„É≥„ÇπÁîüÊàê„Ç§„É≥„Çø„Éº„Éï„Çß„Éº„Çπ"""
        return self.get_provider(provider).generate_response(prompt, **kwargs)

    def generate_image_response(
        self, provider: str, prompt: str, image_path: str, **kwargs
    ) -> LLMResponse:
        """Áµ±‰∏ÄÁîªÂÉè„É¨„Çπ„Éù„É≥„ÇπÁîüÊàê„Ç§„É≥„Çø„Éº„Éï„Çß„Éº„Çπ"""
        return self.get_provider(provider).generate_image_response(prompt, image_path, **kwargs)


# =============================================================================
# ‰ª•‰∏ã„ÄÅmain.py„Å®„ÅÆÂÆåÂÖ®„Å™ÂæåÊñπ‰∫íÊèõÊÄß„ÅÆ„Åü„ÇÅ„ÅÆ„É¨„Ç¨„Ç∑„ÉºÈñ¢Êï∞
# =============================================================================

# „Ç∞„É≠„Éº„Éê„É´Ë®≠ÂÆö„Ç§„É≥„Çπ„Çø„É≥„Çπ
_global_config = LLMConfig()


def encode_image(image_path):
    """„É¨„Ç¨„Ç∑„Éº: ÁîªÂÉè„ÇíBase64„Ç®„É≥„Ç≥„Éº„Éâ"""
    return ImageProcessor.encode_image_to_base64(image_path)


def get_base64_encoded_image(image_path):
    """„É¨„Ç¨„Ç∑„Éº: ÁîªÂÉè„ÇíBase64„Ç®„É≥„Ç≥„Éº„ÉâÔºàÂà•ÂêçÔºâ"""
    return ImageProcessor.encode_image_to_base64(image_path)


def fetch_response(prompt, image_path, model_name="gpt-4o-mini"):
    """„É¨„Ç¨„Ç∑„Éº: OpenAI API„ÅßÁîªÂÉèËß£ÊûêÔºàmain.py„Åß‰ΩøÁî®Ôºâ"""
    log_with_time(f"start fetch_response {model_name}", level="INFO")

    # „É¢„Éá„É´Âêç„Çí‰øÆÊ≠£Ôºàmain.py„Åß„Éè„Éº„Éâ„Ç≥„Éº„Éâ„Åï„Çå„Å¶„ÅÑ„ÇãÔºâ
    model_name = "gpt-4o-2024-11-20"

    base64_image = encode_image(image_path)

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {_global_config.openai_api_key}",
    }

    payload = {
        "model": model_name,
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"},
                    },
                ],
            }
        ],
        "max_tokens": 4000,
    }

    response = requests.post(
        "https://api.openai.com/v1/chat/completions", headers=headers, json=payload
    )
    return response


def fetch_gemini_response(prompt, image_path, model="gemini-2.5-flash"):
    """„É¨„Ç¨„Ç∑„Éº: GeminiÁîªÂÉèÂá¶ÁêÜÔºàmain.py„Åß‰ΩøÁî®Ôºâ"""
    log_with_time(f"start fetch_gemini_response with {model}", level="INFO")

    log_with_time("üåê Using Gemini API", level="INFO")

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    query_dir = f"tmp/api_queries/gemini_{timestamp}"
    os.makedirs(query_dir, exist_ok=True)

    # ÁîªÂÉè„Çíbase64„Ç®„É≥„Ç≥„Éº„Éâ
    base64_image = encode_image(image_path)

    # Gemini„ÇØ„É©„Ç§„Ç¢„É≥„Éà„Çí‰ΩúÊàê
    client = genai.Client(api_key=_global_config.google_api_key)

    # „Éó„É≠„É≥„Éó„Éà„Çí‰øùÂ≠ò
    with open(f"{query_dir}/prompt.txt", "w", encoding="utf-8") as f:
        f.write(prompt)

    # ÂÖÉÁîªÂÉè„Çí„Ç≥„Éî„Éº
    image_filename = os.path.basename(image_path)
    shutil.copy2(image_path, f"{query_dir}/{image_filename}")

    # „ÇØ„Ç®„É™ÊÉÖÂ†±„Çí‰øùÂ≠ò
    query_info = {
        "model": model,
        "timestamp": timestamp,
        "prompt_length": len(prompt),
        "image_path": image_path,
        "image_filename": image_filename,
        "base64_length": len(base64_image),
    }
    with open(f"{query_dir}/query_info.json", "w", encoding="utf-8") as f:
        json.dump(query_info, f, ensure_ascii=False, indent=2)

    log_with_time(f"Query saved to: {query_dir}", level="INFO")

    # ÁîªÂÉè‰ªò„Åç„Éó„É≠„É≥„Éó„Éà„Çí‰ΩúÊàêÔºàPart„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„Çí‰ΩøÁî®Ôºâ
    contents = [
        Part(text=prompt),
        Part(inline_data={"mime_type": "image/jpeg", "data": base64_image}),
    ]

    # „É¨„Çπ„Éù„É≥„Çπ„ÇíÂèñÂæó
    response = client.models.generate_content(model=model, contents=contents)

    return response


def fetch_gemini_response_base64(
    prompt, base64_image, model="gemini-2.5-flash", original_image_path=None
):
    """„É¨„Ç¨„Ç∑„Éº: Gemini Base64ÁîªÂÉèÂá¶ÁêÜÔºàmain.py„Åß‰ΩøÁî®Ôºâ"""
    log_with_time(f"start fetch_gemini_response_base64 with {model}", level="INFO")

    log_with_time("üåê Using Gemini API", level="INFO")

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    query_dir = f"tmp/api_queries/gemini_base64_{timestamp}"
    os.makedirs(query_dir, exist_ok=True)

    # Gemini„ÇØ„É©„Ç§„Ç¢„É≥„Éà„Çí‰ΩúÊàê
    client = genai.Client(api_key=_global_config.google_api_key)

    # base64_image„Åådata:image/jpeg;base64,„ÅßÂßã„Åæ„ÇãÂ†¥Âêà„ÅØÂâäÈô§
    if base64_image.startswith("data:image"):
        base64_image = base64_image.split(",")[1]

    # „Éó„É≠„É≥„Éó„Éà„Çí‰øùÂ≠ò
    with open(f"{query_dir}/prompt.txt", "w", encoding="utf-8") as f:
        f.write(prompt)

    # ÁîªÂÉè„Çí‰øùÂ≠òÔºàBase64„Éá„Ç≥„Éº„Éâ„Åó„Å¶‰øùÂ≠òÔºâ
    import base64 as b64

    image_data = b64.b64decode(base64_image)
    with open(f"{query_dir}/image.jpg", "wb") as f:
        f.write(image_data)

    # „ÇØ„Ç®„É™ÊÉÖÂ†±„Çí‰øùÂ≠ò
    query_info = {
        "model": model,
        "timestamp": timestamp,
        "prompt_length": len(prompt),
        "base64_length": len(base64_image),
        "image_size": len(image_data),
        "original_image_path": original_image_path,
    }
    with open(f"{query_dir}/query_info.json", "w", encoding="utf-8") as f:
        json.dump(query_info, f, ensure_ascii=False, indent=2)

    log_with_time(f"Query saved to: {query_dir}", level="INFO")

    # ÁîªÂÉè‰ªò„Åç„Éó„É≠„É≥„Éó„Éà„Çí‰ΩúÊàêÔºàPart„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„Çí‰ΩøÁî®Ôºâ
    contents = [
        Part(text=prompt),
        Part(inline_data={"mime_type": "image/jpeg", "data": base64_image}),
    ]

    # „É¨„Çπ„Éù„É≥„Çπ„ÇíÂèñÂæó
    response = client.models.generate_content(model=model, contents=contents)

    return response


def fetch_gemini_response_multimodal(prompt_messages, model="gemini-2.5-flash"):
    """„É¨„Ç¨„Ç∑„Éº: Gemini API„ÅßÈÖçÂàóÂΩ¢Âºè„ÅÆ„Éó„É≠„É≥„Éó„Éà„É°„ÉÉ„Çª„Éº„Ç∏„ÇíÂá¶ÁêÜÔºàmain.py„Åß‰ΩøÁî®Ôºâ"""
    log_with_time(f"start fetch_gemini_response_multimodal with {model}", level="INFO")
    log_with_time(f"Processing {len(prompt_messages)} messages", level="INFO")

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    query_dir = f"tmp/api_queries/gemini_multimodal_{timestamp}"
    os.makedirs(query_dir, exist_ok=True)

    # „ÇØ„Ç®„É™ÊÉÖÂ†±„Çí‰øùÂ≠ò
    query_info = {
        "model": model,
        "timestamp": timestamp,
        "message_count": len(prompt_messages),
        "messages": [],
    }

    # Gemini„ÇØ„É©„Ç§„Ç¢„É≥„Éà„Çí‰ΩúÊàê
    client = genai.Client(api_key=_global_config.google_api_key)

    # „Éó„É≠„É≥„Éó„Éà„É°„ÉÉ„Çª„Éº„Ç∏„ÇíGemini„ÅÆÂΩ¢Âºè„Å´Â§âÊèõ
    content_parts = []

    for i, message in enumerate(prompt_messages):
        if message.get("type") == "text":
            # „ÉÜ„Ç≠„Çπ„Éà„ÅÆPart„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„Çí‰ΩúÊàê
            text_content = message["text"]
            content_parts.append(Part(text=text_content))
            log_with_time(f"Message {i}: text ({len(text_content)} chars)", level="INFO")

            # „ÉÜ„Ç≠„Çπ„Éà„Çí„Éï„Ç°„Ç§„É´„Å´‰øùÂ≠ò
            text_file = f"{query_dir}/message_{i}_text.txt"
            with open(text_file, "w", encoding="utf-8") as f:
                f.write(text_content)

            query_info["messages"].append(
                {
                    "index": i,
                    "type": "text",
                    "file": f"message_{i}_text.txt",
                    "length": len(text_content),
                }
            )

        elif message.get("type") == "image_url":
            # image_url„Åã„ÇâÁîªÂÉè„Éá„Éº„Çø„ÇíÊäΩÂá∫
            image_url = message["image_url"]["url"]
            if image_url.startswith("data:image"):
                # "data:image/jpeg;base64," „ÅÆÈÉ®ÂàÜ„ÇíÈô§Âéª
                base64_data = image_url.split(",")[1]
                # ÁîªÂÉè„ÅÆPart„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„Çí‰ΩúÊàê
                content_parts.append(
                    Part(inline_data={"mime_type": "image/jpeg", "data": base64_data})
                )
                log_with_time(
                    f"Message {i}: image (base64, {len(base64_data)} chars)",
                    level="INFO",
                )

                # ÁîªÂÉè„Çí„Éï„Ç°„Ç§„É´„Å´‰øùÂ≠òÔºàBase64„Éá„Ç≥„Éº„Éâ„Åó„Å¶‰øùÂ≠òÔºâ
                import base64 as b64

                image_data = b64.b64decode(base64_data)
                image_file = f"{query_dir}/message_{i}_image.jpg"
                with open(image_file, "wb") as f:
                    f.write(image_data)

                query_info["messages"].append(
                    {
                        "index": i,
                        "type": "image",
                        "file": f"message_{i}_image.jpg",
                        "base64_length": len(base64_data),
                        "file_size": len(image_data),
                    }
                )

            else:
                log_with_time(
                    f"Warning: Unsupported image URL format: {image_url[:50]}...",
                    level="WARNING",
                )
        else:
            log_with_time(
                f"Warning: Unsupported message type: {message.get('type')}",
                level="WARNING",
            )

    # „ÇØ„Ç®„É™ÊÉÖÂ†±„ÇíJSON„Åß‰øùÂ≠ò
    with open(f"{query_dir}/query_info.json", "w", encoding="utf-8") as f:
        json.dump(query_info, f, ensure_ascii=False, indent=2)

    log_with_time(f"Generated {len(content_parts)} content parts for Gemini", level="INFO")
    log_with_time(f"Query saved to: {query_dir}", level="INFO")

    # „É¨„Çπ„Éù„É≥„Çπ„ÇíÂèñÂæó
    try:
        response = client.models.generate_content(model=model, contents=content_parts)
        return response
    except Exception as e:
        log_with_time(f"Error in Gemini API call: {e}", level="ERROR")
        log_with_time(f"Content parts: {len(content_parts)} items", level="ERROR")
        for i, part in enumerate(content_parts):
            log_with_time(f"  Part {i}: {type(part)}", level="ERROR")
            if hasattr(part, "text"):
                log_with_time(f"    - text: {len(part.text)} chars", level="ERROR")
            if hasattr(part, "inline_data"):
                log_with_time(
                    f"    - inline_data: {part.inline_data.keys() if isinstance(part.inline_data, dict) else 'present'}",
                    level="ERROR",
                )
        raise


def fetch_gemini_cli_response(prompt, model="gemini-2.5-flash"):
    """„É¨„Ç¨„Ç∑„Éº: Gemini API„ÅßCLI„Åã„Çâ„ÅÆ„Éó„É≠„É≥„Éó„Éà„ÇíÂá¶ÁêÜÔºàmain.py„Åß‰ΩøÁî®Ôºâ"""
    log_with_time(f"start fetch_gemini_cli_response with {model}", level="INFO")
    client = genai.Client(api_key=_global_config.google_api_key)
    response = client.models.generate_content(model=model, contents=prompt)
    return response


def fetch_gemini_4koma_response(prompt, image_path_list, image_data, model="gemini-2.5-flash"):
    """„É¨„Ç¨„Ç∑„Éº: Gemini API„Åß4„Ç≥„ÉûÂÖ®‰Ωì„ÇíÂá¶ÁêÜÔºàmain.py„Åß‰ΩøÁî®Ôºâ"""
    log_with_time(f"start fetch_gemini_4koma_response with {model}", level="INFO")

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    query_dir = f"tmp/api_queries/gemini_4koma_{timestamp}"
    os.makedirs(query_dir, exist_ok=True)

    # Gemini„ÇØ„É©„Ç§„Ç¢„É≥„Éà„Çí‰ΩúÊàê
    client = genai.Client(api_key=_global_config.google_api_key)

    # Ë§áÊï∞ÁîªÂÉè„Å®„Éó„É≠„É≥„Éó„Éà„Çí‰ΩúÊàêÔºàPart„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„Çí‰ΩøÁî®Ôºâ
    contents = []

    # „ÇØ„Ç®„É™ÊÉÖÂ†±„Çí‰øùÂ≠ò
    query_info = {
        "model": model,
        "timestamp": timestamp,
        "prompt": prompt,
        "image_count": len(image_path_list),
        "images": [],
    }

    # ÂêÑÁîªÂÉè„ÇíËøΩÂä†
    for i, image_path in enumerate(image_path_list):
        contents.append(Part(text=f"Image {i + 1}:"))
        base64_image = get_base64_encoded_image("public" + image_path)
        contents.append(Part(inline_data={"mime_type": "image/jpeg", "data": base64_image}))

        # ÁîªÂÉè„Çí‰øùÂ≠òÔºàBase64„Éá„Ç≥„Éº„Éâ„Åó„Å¶‰øùÂ≠òÔºâ
        import base64 as b64

        image_data_bytes = b64.b64decode(base64_image)
        image_file = f"{query_dir}/image_{i + 1}.jpg"
        with open(image_file, "wb") as f:
            f.write(image_data_bytes)

        query_info["images"].append(
            {
                "index": i + 1,
                "path": image_path,
                "file": f"image_{i + 1}.jpg",
                "size": len(image_data_bytes),
            }
        )

    # imageData„Å®„Éó„É≠„É≥„Éó„Éà„ÇíËøΩÂä†
    contents.append(Part(text=json.dumps(image_data)))
    contents.append(Part(text=prompt))

    # „Éó„É≠„É≥„Éó„Éà„Çí‰øùÂ≠ò
    with open(f"{query_dir}/prompt.txt", "w", encoding="utf-8") as f:
        f.write(prompt)

    # imageData„Çí‰øùÂ≠ò
    with open(f"{query_dir}/image_data.json", "w", encoding="utf-8") as f:
        json.dump(image_data, f, ensure_ascii=False, indent=2)

    # „ÇØ„Ç®„É™ÊÉÖÂ†±„Çí‰øùÂ≠ò
    with open(f"{query_dir}/query_info.json", "w", encoding="utf-8") as f:
        json.dump(query_info, f, ensure_ascii=False, indent=2)

    log_with_time(f"Query saved to: {query_dir}", level="INFO")

    # „É¨„Çπ„Éù„É≥„Çπ„ÇíÂèñÂæó
    response = client.models.generate_content(model=model, contents=contents)

    return response


def fetch_gemini_discussion_response(prompt, summary, image_data, model="gemini-2.5-flash"):
    """„É¨„Ç¨„Ç∑„Éº: Gemini API„Åß„Éá„Ç£„Çπ„Ç´„ÉÉ„Ç∑„Éß„É≥Âá¶ÁêÜÔºàmain.py„Åß‰ΩøÁî®Ôºâ"""
    log_with_time(f"start fetch_gemini_discussion_response with {model}", level="INFO")

    # Gemini„ÇØ„É©„Ç§„Ç¢„É≥„Éà„Çí‰ΩúÊàê
    client = genai.Client(api_key=_global_config.google_api_key)

    # „Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÇíÊßãÁØâ
    contents = []

    # summary„Åå„ÉÜ„Ç≠„Çπ„Éà„ÅÆÂ†¥Âêà„Å®„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„ÅÆÂ†¥Âêà„ÅÆ‰∏°Êñπ„Å´ÂØæÂøú
    if isinstance(summary, str):
        summary_text = summary
    elif isinstance(summary, dict) and "content" in summary:
        # Claude„ÅÆ„É¨„Çπ„Éù„É≥„ÇπÂΩ¢Âºè„ÅÆÂ†¥Âêà
        summary_text = (
            summary["content"][0]["text"]
            if isinstance(summary["content"], list)
            else summary["content"]
        )
    else:
        summary_text = json.dumps(summary)

    # ‰ºöË©±„ÅÆÊµÅ„Çå„ÇíÊßãÁØâ
    contents.append(Part(text="‰ª•‰∏ã„ÅÆ4„Ç≥„ÉûÊº´Áîª„Å´„Å§„ÅÑ„Å¶Ë©±„ÅóÂêà„ÅÑ„Åæ„Åó„Çá„ÅÜ„ÄÇ"))
    contents.append(Part(text=f"ÁîªÂÉè„Éá„Éº„Çø: {json.dumps(image_data)}"))
    contents.append(Part(text=f"4„Ç≥„ÉûÂÖ®‰Ωì„ÅÆ„Åæ„Å®„ÇÅ: {summary_text}"))
    contents.append(Part(text=f"Ë≥™Âïè: {prompt}"))

    # „É¨„Çπ„Éù„É≥„Çπ„ÇíÂèñÂæó
    response = client.models.generate_content(model=model, contents=contents)

    return response


def fetch_anthropic_4koma_response(prompt, image_path_list, image_data):
    """„É¨„Ç¨„Ç∑„Éº: Claude API„Åß4„Ç≥„ÉûÂÖ®‰Ωì„ÇíÂá¶ÁêÜÔºàmain.py„Åß‰ΩøÁî®Ôºâ"""
    MODEL_NAME = "claude-3-5-sonnet-20240620"
    log_with_time(f"start fetch_anthropic_4koma_response {MODEL_NAME}", level="INFO")
    message_list = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "Image 1:"},
                {
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": "image/jpeg",
                        "data": get_base64_encoded_image("public" + image_path_list[0]),
                    },
                },
                {"type": "text", "text": "Image 2:"},
                {
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": "image/jpeg",
                        "data": get_base64_encoded_image("public" + image_path_list[1]),
                    },
                },
                {"type": "text", "text": "Image 3:"},
                {
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": "image/jpeg",
                        "data": get_base64_encoded_image("public" + image_path_list[2]),
                    },
                },
                {"type": "text", "text": "Image 4:"},
                {
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": "image/jpeg",
                        "data": get_base64_encoded_image("public" + image_path_list[3]),
                    },
                },
                {"type": "text", "text": json.dumps(image_data)},
                {"type": "text", "text": prompt},
            ],
        },
    ]
    client = anthropic.Anthropic(
        api_key=_global_config.anthropic_api_key,
    )
    response = client.messages.create(model=MODEL_NAME, max_tokens=4048, messages=message_list)
    return response


def fetch_anthropic_discussion_response(prompt, summary, image_data):
    """„É¨„Ç¨„Ç∑„Éº: Claude API„Åß„Éá„Ç£„Çπ„Ç´„ÉÉ„Ç∑„Éß„É≥Âá¶ÁêÜÔºàmain.py„Åß‰ΩøÁî®Ôºâ"""
    MODEL_NAME = "claude-3-5-sonnet-20240620"
    log_with_time(f"start fetch_anthropic_discussion_response {MODEL_NAME}", level="INFO")
    assistant_data = {}
    assistant_data = {k: v for k, v in summary.items() if k in ["role", "content"]}

    message_list = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": json.dumps(image_data)},
                {
                    "type": "text",
                    "text": "ÁîªÂÉèÈÉ°„Å®imageData„Åã„Çâ4„Ç≥„ÉûÂÖ®‰Ωì„Åß„Å©„Çì„Å™Ë©±„Åã„Çí„Åæ„Å®„ÇÅ„Å¶„Åè„Å†„Åï„ÅÑ",
                },
            ],
        },
        assistant_data,
        {"role": "user", "content": [{"type": "text", "text": prompt}]},
    ]
    client = anthropic.Anthropic(
        api_key=_global_config.anthropic_api_key,
    )
    response = client.messages.create(model=MODEL_NAME, max_tokens=4048, messages=message_list)
    return response


def get_gpt_response(prompt: str, image_path: str, model_name: str = "gpt-4o-2024-11-20") -> Dict:
    """„É¨„Ç¨„Ç∑„Éº: GPT„É¨„Çπ„Éù„É≥„ÇπÂèñÂæóÔºàÂæåÊñπ‰∫íÊèõÊÄßÔºâ"""
    manager = LLMManager()
    response = manager.generate_image_response("openai", prompt, image_path, model=model_name)
    return {"choices": [{"message": {"content": response.content}}]}


def get_claude_response(prompt: str, image_path: str = None) -> Dict:
    """„É¨„Ç¨„Ç∑„Éº: Claude„É¨„Çπ„Éù„É≥„ÇπÂèñÂæóÔºàÂæåÊñπ‰∫íÊèõÊÄßÔºâ"""
    manager = LLMManager()
    if image_path:
        response = manager.generate_image_response("anthropic", prompt, image_path)
    else:
        response = manager.generate_response("anthropic", prompt)
    return {"content": [{"text": response.content}]}


def get_gemini_response(prompt: str, model: str = "gemini-2.5-flash") -> str:
    """„É¨„Ç¨„Ç∑„Éº: Gemini„É¨„Çπ„Éù„É≥„ÇπÂèñÂæóÔºàÂæåÊñπ‰∫íÊèõÊÄßÔºâ"""
    manager = LLMManager()
    response = manager.generate_response("gemini", prompt, model=model)
    return response.content


def print_and_save_response(response, koma_id):
    """„É¨„Ç¨„Ç∑„Éº: „É¨„Çπ„Éù„É≥„Çπ„ÅÆË°®Á§∫„Å®‰øùÂ≠ò"""
    save_dir = "chat_results"
    os.makedirs(name=save_dir, exist_ok=True)
    log_with_time(f"Response attributes: {dir(response)}", level="DEBUG")
    log_with_time(f"Response content: {response.json()['content'][0]['text']}", level="DEBUG")

    with open(save_dir + "/" + koma_id + ".json", "w") as f:
        f.write(response.text)


def save_response(response, image_path):
    """„É¨„Ç¨„Ç∑„Éº: „É¨„Çπ„Éù„É≥„Çπ„Çí„Éï„Ç°„Ç§„É´„Å´ËøΩË®ò"""
    p = Path(image_path)
    with open(f"chat_results/result.txt", "+a") as f:
        f.write(f"{image_path}___{response.json()}\n")


def build_koma_id(image_path):
    """„É¨„Ç¨„Ç∑„Éº: ÁîªÂÉè„Éë„Çπ„Åã„Çâ„Ç≥„ÉûID„ÇíÁîüÊàê"""
    data_name = image_path.split("/")[1]
    kanji = data_name[-2:]
    page_koma_num = RE_PAGE_KOMA_NUM.findall(image_path)[-1]
    koma_id = f"{kanji}-{page_koma_num}"
    return koma_id


# „Éó„É≠„É≥„Éó„Éà„ÉÜ„É≥„Éó„É¨„Éº„Éà
prompt_anal_koma = """
„Äå„Çè„Åã„Çâ„Å™„ÅÑ„Åì„Å®„ÅØÁü•„Çâ„Å™„ÅÑ„Å®Á≠î„Åà„Å¶„Äç
‰ª•‰∏ã„ÅØ„Äé„ÇÜ„ÇÜÂºè„Äè„ÅÆÁîªÂÉè„Åß„Åô„ÄÇÊó•Êú¨„ÅÆ4„Ç≥„ÉûÊº´Áîª„ÅÆ„Åü„ÇÅ„ÄÅÂè≥„Åã„ÇâÂ∑¶„Å´Ë™≠„Çì„Åß„ÅÑ„Åç„Åæ„Åô„ÄÇ‰∏ª„Å™ÁôªÂ†¥‰∫∫Áâ©„ÅØ‰ª•‰∏ã„Åß„Åô„ÄÇ

- Êó•ÂêëÁ∏Å
	- "Á∏Å„ÅÆÈ´™„ÅÆÁâπÂæ¥": [  "Èªí„Å£„ÅΩ„ÅÑÈ´™Ëâ≤",  "Èï∑„ÇÅÔºàËÇ©„Åè„Çâ„ÅÑ„ÅÆÈï∑„ÅïÔºâ",  "„Çπ„Éà„É¨„Éº„Éà„Å™È´™Ë≥™",  "ÂâçÈ´™„Åå„ÅÇ„Çã"  ]
- Ê´ü‰∫ïÂîØ 
	- "ÂîØ„ÅÆÈ´™„ÅÆÁâπÂæ¥": [  "ÁôΩ„Å£„ÅΩ„ÅÑÈ´™Ëâ≤",  "Áü≠„ÇÅÔºàÈ¶ñ„ÅÇ„Åü„Çä„ÅÆÈï∑„ÅïÔºâ",  "„ÇÑ„ÇÑ„Ç¶„Çß„Éº„Éñ„Åå„Åã„Åã„Å£„Å¶„ÅÑ„Çã„Çà„ÅÜ„Å´Ë¶ã„Åà„Çã",  "ÂâçÈ´™„Åå„ÅÇ„Çã",  "È´™„ÅÆÂÖàÁ´Ø„ÅåÂ∞ë„ÅóÂ§ñÂÅ¥„Å´Ë∑≥„Å≠„Å¶„ÅÑ„Çã"  ]
- Èáé„ÄÖÂéü„ÇÜ„Åö„Åì 
	- "„ÇÜ„Åö„Åì„ÅÆÈ´™„ÅÆÁâπÂæ¥": [ "Áü≠„ÇÅ",  "„Éú„Éñ„Ç´„ÉÉ„Éà„ÅÆ„Çà„ÅÜ„Å™„Çπ„Çø„Ç§„É´",  "ÂâçÈ´™„Åå„ÇÑ„ÇÑÈï∑„ÇÅ",  "È´™„ÅÆÂÖàÁ´Ø„ÅåÂÜÖÂÅ¥„Å´Â∞ë„ÅóÂ∑ª„ÅçËæº„Çì„Åß„ÅÑ„Çã",  "È´™Ëâ≤„ÅØÁôΩÈ´™„Åß„ÇÇÈªíÈ´™„Åß„ÇÇ„Å™„ÅÑ‰∏≠ÈñìÁöÑ„Å™Ëâ≤Ôºà„Åä„Åù„Çâ„Åè„Ç∞„É¨„Éº„Åå„Åã„Å£„ÅüËâ≤Ôºâ"  ]
„Åì„ÅÆÁîªÂÉè„Å´„Å§„ÅÑ„Å¶‰ª•‰∏ã„Å´Á≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ„Åù„Çå„Åû„Çå„Å´„Å§„ÅÑ„Å¶ÁîªÂÉè„ÅÆÂè≥ÂÅ¥„Å´„ÅÇ„Çã„ÇÇ„ÅÆ„Åã„ÇâÈ†Ü„Å´Á≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
- „ÇÜ„Åö„Åì„Åü„Å°„ÅØ„ÄÅ„Åù„Çå„Åû„Çå„Åå„Ç≥„Éû„ÅÆ„Å©„Åì„Å´„ÅÑ„Åæ„Åô„ÅãÔºüË™∞„ÇÇ„ÅÑ„Å™„ÅÑÂ†¥Âêà„ÇÇ„ÅÇ„Çã„Åó„ÄÅ3‰∫∫„Çà„ÇäÂ§ö„ÅÑ‰∫∫Êï∞„ÅåÂÜô„Å£„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÇÇ„ÅÇ„Çä„Åæ„Åô
	- ÁîªÂÉè„ÅÆÂè≥ÂÅ¥„Å´„ÅÇ„Çã„ÇÇ„ÅÆ„Åã„ÇâÈ†Ü„Å´Á≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
	- Â∫ßÊ®ô„ÇÇÁ≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ
- „Å©„Çì„Å™Ë°®ÊÉÖ„Çí„Åó„Å¶„ÅÑ„Åæ„Åô„Åã?„ÅÑ„Å™„ÅÑÂ†¥Âêà„ÅØ„Äå„ÅÑ„Å™„ÅÑ„Äç
- È°î„ÅÆÂêë„Åç„ÇíÁ≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ.„ÅÑ„Å™„ÅÑÂ†¥Âêà„ÅØ„Äå„ÅÑ„Å™„ÅÑ„Äç
- „Å©„Çì„Å™„Çª„É™„Éï„ÇíË®Ä„Å£„Å¶„ÅÑ„Åæ„Åô„ÅãÔºü„Å™„ÅÑÂ†¥Âêà„ÅØ„Äå„Å™„Åó„Äç
	- ÁîªÂÉè„ÅÆÂè≥ÂÅ¥„Å´„ÅÇ„Çã„ÇÇ„ÅÆ„Åã„ÇâÈ†Ü„Å´Á≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
	- Â∫ßÊ®ô„ÇÇÁîªÂÉè„Å´ÂØæ„Åô„ÇãÊØîÁéá„ÅßÁ≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ „ÅßÁ≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ 
	- Êñá„ÅåË§áÊï∞„ÅÇ„ÇãÂ†¥Âêà„ÅØÂçäËßí„Çπ„Éö„Éº„Çπ„ÅßÂå∫Âàá„Å£„Å¶„Åè„Å†„Åï„ÅÑ
- „Å©„Çì„Å™ÊúçË£Ö„Çí„Åó„Å¶„ÅÑ„Åæ„Åô„ÅãÔºü„ÅÑ„Å™„ÅÑÂ†¥Âêà„ÅØ„Äå„ÅÑ„Å™„ÅÑ„Äç
- „Å©„Çì„Å™Â†¥Èù¢„Å†„Å®ÊÄù„Çè„Çå„Åæ„Åô„ÅãÔºü„ÅÑ„Å™„ÅÑÂ†¥Âêà„ÅØ„Äå„ÅÑ„Å™„ÅÑ„Äç
- Ë©±„Åó„Å¶„ÅÑ„ÇãÂ†¥ÊâÄ„ÅØ„Å©„Åì„Åß„Åô„ÅãÔºü„Çè„Åã„Çâ„Å™„ÅÑÂ†¥Âêà„ÅØ„Äå‰∏çÊòé„Äç
- ËÉåÊôØ„Å´Êº´Áîª„Å™„Çâ„Åß„ÅØ„ÅÆÂäπÊûúÊèèÂÜô„Åå„ÅÇ„Çå„Å∞Êïô„Åà„Å¶‰∏ã„Åï„ÅÑ„ÄÇ„Å™„ÅÑÂ†¥Âêà„ÅØ„Äå„Å™„Åó„Äç
- ÁîªÂÉè„ÅÆ‰∏≠„Å´„ÅØÂÖ®ÈÉ®„Åß‰Ωï‰∫∫„ÅÑ„Åæ„Åô„Åã
- ÁîªÂÉè„ÅÆ‰∏≠„Å´„ÅØÂêπ„ÅçÂá∫„Åó„Åå„ÅÑ„Åè„Å§„ÅÇ„Çä„Åæ„Åô„Åã
- Âá∫Âäõ„ÅØJSONÂΩ¢Âºè„ÅÆ„Åø

Âá∫Âäõ„Åô„Çãjson„ÅØ‰ª•‰∏ã„ÅÆÂΩ¢Âºè„Å´„Åó„Å¶„Åè„Å†„Åï„ÅÑ
```
{
    charactersNum: int,
    serifsNum: int,
    characters: [
    {
        character: "",
        faceDirection: "",
        position: "",
        expression: "",
        serif: "",
        clothing: "",
        isVisible: true,
    },
    {
        character: "",
        faceDirection: "",
        position: "",
        expression: "",
        serif: "",
        clothing: "",
        isVisible: true,
    },
    {
        character: "",
        faceDirection: "",
        position: "",
        expression: "",
        serif: "",
        clothing: "",
        isVisible: true,
    },
    ],
    sceneData: {scene: "", location: "", backgroundEffects: ""},
}
```
"""


# „Éó„É≠„É≥„Éó„Éà„ÉÜ„É≥„Éó„É¨„Éº„ÉàÔºà„ÇØ„É©„ÇπÁâàÔºâ
class PromptTemplates:
    """4„Ç≥„ÉûÊº´ÁîªËß£ÊûêÁî®„Éó„É≠„É≥„Éó„Éà„ÉÜ„É≥„Éó„É¨„Éº„Éà"""

    KOMA_ANALYSIS = prompt_anal_koma


def save_response_to_file(response: LLMResponse, output_path: str) -> None:
    """„É¨„Çπ„Éù„É≥„Çπ„Çí„Éï„Ç°„Ç§„É´„Å´‰øùÂ≠ò"""
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(
            {
                "content": response.content,
                "model": response.model,
                "provider": response.provider,
                "timestamp": response.timestamp.isoformat(),
                "metadata": response.metadata,
            },
            f,
            ensure_ascii=False,
            indent=2,
        )
