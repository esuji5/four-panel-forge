#!/usr/bin/env python3
"""
LLM Utils - å®Œå…¨ç‰ˆï¼ˆmain.pyã¨ã®äº’æ›æ€§ã‚’ä¿æŒï¼‰

4ã‚³ãƒæ¼«ç”»è§£æã®ãŸã‚ã®LLMçµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
æ—¢å­˜ã®llm_utils.pyã¨ã®å®Œå…¨ãªå¾Œæ–¹äº’æ›æ€§ã‚’æä¾›
"""

import base64
import json
import os
import re
import shutil
import subprocess
import sys
from abc import ABC, abstractmethod
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

import anthropic
import google.genai as genai
import requests
from dotenv import load_dotenv
from google.genai.types import Part

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()


# ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ­ã‚°é–¢æ•°
def log_with_time(message: str, level: str = "INFO"):
    """ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãã§ãƒ­ã‚°ã‚’å‡ºåŠ›"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
    prefix = {
        "INFO": "â„¹ï¸",
        "DEBUG": "ğŸ”",
        "ERROR": "âŒ",
        "WARNING": "âš ï¸",
        "SUCCESS": "âœ…",
    }.get(level, "ğŸ“")

    log_message = f"[{timestamp}] {prefix} {message}"
    print(log_message)
    sys.stdout.flush()  # å³åº§ã«å‡ºåŠ›ã‚’åæ˜ 


def load_config() -> Dict[str, Any]:
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆç’°å¢ƒåˆ¥ï¼‰"""
    env = os.getenv("ENVIRONMENT", "local")
    config_dir = Path("config")

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’èª­ã¿è¾¼ã¿
    default_config = {}
    default_file = config_dir / "default.json"
    if default_file.exists():
        with open(default_file, "r", encoding="utf-8") as f:
            default_config = json.load(f)

    # ç’°å¢ƒåˆ¥è¨­å®šã‚’èª­ã¿è¾¼ã¿
    env_config = {}
    env_file = config_dir / f"{env}.json"
    if env_file.exists():
        with open(env_file, "r", encoding="utf-8") as f:
            env_config = json.load(f)

    # è¨­å®šã‚’ãƒãƒ¼ã‚¸ï¼ˆç’°å¢ƒè¨­å®šãŒå„ªå…ˆï¼‰
    def merge_dict(base: dict, override: dict) -> dict:
        result = base.copy()
        for key, value in override.items():
            if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                result[key] = merge_dict(result[key], value)
            else:
                result[key] = value
        return result

    return merge_dict(default_config, env_config)


# æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³
RE_PAGE_KOMA_NUM = re.compile(r"\b\d{3}-\d\b")


@dataclass
class LLMConfig:
    """LLMè¨­å®šã‚¯ãƒ©ã‚¹ï¼ˆç’°å¢ƒåˆ¥è¨­å®šå¯¾å¿œï¼‰"""

    openai_api_key: str = os.getenv("OPENAI_API_KEY", "")
    anthropic_api_key: str = os.getenv("ANTHROPIC_API_KEY", "")
    google_api_key: str = os.getenv("GOOGLE_API_KEY", "")

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§ä¸Šæ›¸ãå¯èƒ½ï¼‰
    openai_model: str = "gpt-4o-2024-11-20"
    anthropic_model: str = "claude-3-5-sonnet-20240620"
    gemini_model: str = "gemini-2.5-flash"

    # ãã®ä»–è¨­å®š
    max_tokens: int = 4048
    confidence_threshold: float = 0.5
    temp_dir: str = "tmp/api_queries"

    def __post_init__(self):
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å€¤ã‚’èª­ã¿è¾¼ã¿"""
        config = load_config()

        # LLMè¨­å®šã‚’èª­ã¿è¾¼ã¿
        llm_config = config.get("llm", {})

        # Geminiè¨­å®šã®ä¸Šæ›¸ã
        gemini_config = llm_config.get("gemini", {})
        if "model" in gemini_config:
            self.gemini_model = gemini_config["model"]

        # OpenAIè¨­å®šã®ä¸Šæ›¸ã
        openai_config = llm_config.get("openai", {})
        if "model" in openai_config:
            self.openai_model = openai_config["model"]
        if "max_tokens" in openai_config:
            self.max_tokens = openai_config["max_tokens"]

        # Anthropicè¨­å®šã®ä¸Šæ›¸ã
        anthropic_config = llm_config.get("anthropic", {})
        if "model" in anthropic_config:
            self.anthropic_model = anthropic_config["model"]

        # ã‚¢ãƒ—ãƒªè¨­å®šã®ä¸Šæ›¸ã
        app_config = config.get("app", {})
        if "temp_dir" in app_config:
            self.temp_dir = app_config["temp_dir"]


@dataclass
class LLMResponse:
    """LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹çµ±ä¸€ã‚¯ãƒ©ã‚¹"""

    content: str
    model: str
    provider: str
    timestamp: datetime
    metadata: Dict[str, Any]
    raw_response: Any


class ImageProcessor:
    """ç”»åƒå‡¦ç†ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£"""

    @staticmethod
    def encode_image_to_base64(image_path: Union[str, Path]) -> str:
        """ç”»åƒã‚’Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode("utf-8")

    @staticmethod
    def clean_base64_data(base64_data: str) -> str:
        """Base64ãƒ‡ãƒ¼ã‚¿ã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’é™¤å»"""
        if base64_data.startswith("data:image"):
            return base64_data.split(",")[1]
        return base64_data

    @staticmethod
    def save_base64_image(base64_data: str, output_path: Union[str, Path]) -> None:
        """Base64ãƒ‡ãƒ¼ã‚¿ã‚’ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜"""
        cleaned_data = ImageProcessor.clean_base64_data(base64_data)
        image_bytes = base64.b64decode(cleaned_data)
        with open(output_path, "wb") as f:
            f.write(image_bytes)


class QueryLogger:
    """APIã‚¯ã‚¨ãƒªã®ãƒ­ã‚°æ©Ÿèƒ½"""

    def __init__(self, config: LLMConfig):
        self.config = config

    def create_query_dir(self, provider: str, suffix: str = "") -> Path:
        """ã‚¯ã‚¨ãƒªä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
        query_dir = Path(self.config.temp_dir) / f"{provider}_{suffix}_{timestamp}"
        query_dir.mkdir(parents=True, exist_ok=True)
        return query_dir

    def save_query_info(self, query_dir: Path, query_info: Dict[str, Any]) -> None:
        """ã‚¯ã‚¨ãƒªæƒ…å ±ã‚’ä¿å­˜"""
        with open(query_dir / "query_info.json", "w", encoding="utf-8") as f:
            json.dump(query_info, f, ensure_ascii=False, indent=2)

    def save_prompt(self, query_dir: Path, prompt: str) -> None:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¿å­˜"""
        with open(query_dir / "prompt.txt", "w", encoding="utf-8") as f:
            f.write(prompt)


class BaseLLMProvider(ABC):
    """LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""

    def __init__(self, config: LLMConfig):
        self.config = config
        self.logger = QueryLogger(config)

    @abstractmethod
    def generate_response(self, prompt: str, **kwargs) -> LLMResponse:
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆã®æŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰"""
        pass

    @abstractmethod
    def generate_image_response(self, prompt: str, image_path: str, **kwargs) -> LLMResponse:
        """ç”»åƒä»˜ããƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆã®æŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰"""
        pass


class OpenAIProvider(BaseLLMProvider):
    """OpenAI APIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼"""

    def __init__(self, config: LLMConfig):
        super().__init__(config)
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {config.openai_api_key}",
        }

    def generate_response(self, prompt: str, **kwargs) -> LLMResponse:
        """ãƒ†ã‚­ã‚¹ãƒˆãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆ"""
        model = kwargs.get("model", self.config.openai_model)
        max_tokens = kwargs.get("max_tokens", self.config.max_tokens)

        payload = {
            "model": model,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": max_tokens,
        }

        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=self.headers,
            json=payload,
        )
        response.raise_for_status()

        result = response.json()
        content = result["choices"][0]["message"]["content"]

        return LLMResponse(
            content=content,
            model=model,
            provider="openai",
            timestamp=datetime.now(),
            metadata={"tokens_used": result.get("usage", {})},
            raw_response=result,
        )

    def generate_image_response(self, prompt: str, image_path: str, **kwargs) -> LLMResponse:
        """ç”»åƒä»˜ããƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆ"""
        model = kwargs.get("model", self.config.openai_model)
        max_tokens = kwargs.get("max_tokens", self.config.max_tokens)

        # ç”»åƒã‚’Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        base64_image = ImageProcessor.encode_image_to_base64(image_path)

        payload = {
            "model": model,
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"},
                        },
                    ],
                }
            ],
            "max_tokens": max_tokens,
        }

        # ã‚¯ã‚¨ãƒªãƒ­ã‚°ä¿å­˜
        query_dir = self.logger.create_query_dir("openai", "image")
        self.logger.save_prompt(query_dir, prompt)
        shutil.copy2(image_path, query_dir / f"image{Path(image_path).suffix}")

        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=self.headers,
            json=payload,
        )
        response.raise_for_status()

        result = response.json()
        content = result["choices"][0]["message"]["content"]

        return LLMResponse(
            content=content,
            model=model,
            provider="openai",
            timestamp=datetime.now(),
            metadata={
                "tokens_used": result.get("usage", {}),
                "query_dir": str(query_dir),
            },
            raw_response=result,
        )


class AnthropicProvider(BaseLLMProvider):
    """Anthropic Claude APIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼"""

    def __init__(self, config: LLMConfig):
        super().__init__(config)
        self.client = anthropic.Anthropic(api_key=config.anthropic_api_key)

    def generate_response(self, prompt: str, **kwargs) -> LLMResponse:
        """ãƒ†ã‚­ã‚¹ãƒˆãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆ"""
        model = kwargs.get("model", self.config.anthropic_model)
        max_tokens = kwargs.get("max_tokens", self.config.max_tokens)

        response = self.client.messages.create(
            model=model,
            max_tokens=max_tokens,
            messages=[{"role": "user", "content": prompt}],
        )

        content = response.content[0].text

        return LLMResponse(
            content=content,
            model=model,
            provider="anthropic",
            timestamp=datetime.now(),
            metadata={
                "usage": {
                    "input_tokens": response.usage.input_tokens,
                    "output_tokens": response.usage.output_tokens,
                }
            },
            raw_response=response,
        )

    def generate_image_response(self, prompt: str, image_path: str, **kwargs) -> LLMResponse:
        """ç”»åƒä»˜ããƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆ"""
        model = kwargs.get("model", self.config.anthropic_model)
        max_tokens = kwargs.get("max_tokens", self.config.max_tokens)

        # ç”»åƒã‚’Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        base64_image = ImageProcessor.encode_image_to_base64(image_path)

        # ã‚¯ã‚¨ãƒªãƒ­ã‚°ä¿å­˜
        query_dir = self.logger.create_query_dir("anthropic", "image")
        self.logger.save_prompt(query_dir, prompt)
        shutil.copy2(image_path, query_dir / f"image{Path(image_path).suffix}")

        message_content = [
            {"type": "text", "text": prompt},
            {
                "type": "image",
                "source": {
                    "type": "base64",
                    "media_type": "image/jpeg",
                    "data": base64_image,
                },
            },
        ]

        response = self.client.messages.create(
            model=model,
            max_tokens=max_tokens,
            messages=[{"role": "user", "content": message_content}],
        )

        content = response.content[0].text

        return LLMResponse(
            content=content,
            model=model,
            provider="anthropic",
            timestamp=datetime.now(),
            metadata={
                "usage": {
                    "input_tokens": response.usage.input_tokens,
                    "output_tokens": response.usage.output_tokens,
                },
                "query_dir": str(query_dir),
            },
            raw_response=response,
        )

    def generate_4koma_response(
        self, prompt: str, image_paths: List[str], image_data: Dict, **kwargs
    ) -> LLMResponse:
        """4ã‚³ãƒå…¨ä½“è§£æ"""
        model = kwargs.get("model", self.config.anthropic_model)
        max_tokens = kwargs.get("max_tokens", self.config.max_tokens)

        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ§‹ç¯‰
        message_content = []

        # å„ç”»åƒã‚’è¿½åŠ 
        for i, image_path in enumerate(image_paths):
            message_content.append({"type": "text", "text": f"Image {i + 1}:"})
            base64_image = ImageProcessor.encode_image_to_base64("public" + image_path)
            message_content.append(
                {
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": "image/jpeg",
                        "data": base64_image,
                    },
                }
            )

        # ç”»åƒãƒ‡ãƒ¼ã‚¿ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ 
        message_content.append({"type": "text", "text": json.dumps(image_data)})
        message_content.append({"type": "text", "text": prompt})

        # ã‚¯ã‚¨ãƒªãƒ­ã‚°ä¿å­˜
        query_dir = self.logger.create_query_dir("anthropic", "4koma")
        self.logger.save_prompt(query_dir, prompt)

        response = self.client.messages.create(
            model=model,
            max_tokens=max_tokens,
            messages=[{"role": "user", "content": message_content}],
        )

        content = response.content[0].text

        return LLMResponse(
            content=content,
            model=model,
            provider="anthropic",
            timestamp=datetime.now(),
            metadata={
                "usage": {
                    "input_tokens": response.usage.input_tokens,
                    "output_tokens": response.usage.output_tokens,
                },
                "query_dir": str(query_dir),
            },
            raw_response=response,
        )


class GeminiProvider(BaseLLMProvider):
    """Google Gemini APIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼"""

    def __init__(self, config: LLMConfig):
        super().__init__(config)
        self.client = genai.Client(api_key=config.google_api_key)

    def generate_response(self, prompt: str, **kwargs) -> LLMResponse:
        """ãƒ†ã‚­ã‚¹ãƒˆãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆ"""
        model = kwargs.get("model", self.config.gemini_model)

        response = self.client.models.generate_content(model=model, contents=prompt)

        return LLMResponse(
            content=response.text,
            model=model,
            provider="gemini",
            timestamp=datetime.now(),
            metadata={"candidates": len(response.candidates)},
            raw_response=response,
        )

    def generate_image_response(self, prompt: str, image_path: str, **kwargs) -> LLMResponse:
        """ç”»åƒä»˜ããƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆ"""
        model = kwargs.get("model", self.config.gemini_model)

        log_with_time("ğŸŒ Using Gemini API", level="INFO")
        # ç”»åƒã‚’Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        base64_image = ImageProcessor.encode_image_to_base64(image_path)

        # ã‚¯ã‚¨ãƒªãƒ­ã‚°ä¿å­˜
        query_dir = self.logger.create_query_dir("gemini", "image")
        self.logger.save_prompt(query_dir, prompt)
        shutil.copy2(image_path, query_dir / f"image{Path(image_path).suffix}")

        # Partã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ç”»åƒä»˜ãã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ä½œæˆ
        contents = [
            Part(text=prompt),
            Part(inline_data={"mime_type": "image/jpeg", "data": base64_image}),
        ]

        response = self.client.models.generate_content(model=model, contents=contents)

        return LLMResponse(
            content=response.text,
            model=model,
            provider="gemini",
            timestamp=datetime.now(),
            metadata={
                "candidates": len(response.candidates),
                "query_dir": str(query_dir),
            },
            raw_response=response,
        )

    def generate_multimodal_response(self, messages: List[Dict], **kwargs) -> LLMResponse:
        """ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†"""
        model = kwargs.get("model", self.config.gemini_model)

        # ã‚¯ã‚¨ãƒªãƒ­ã‚°ä¿å­˜
        query_dir = self.logger.create_query_dir("gemini", "multimodal")

        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’Partã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
        content_parts = []
        for i, message in enumerate(messages):
            if message.get("type") == "text":
                content_parts.append(Part(text=message["text"]))

                # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                with open(query_dir / f"message_{i}_text.txt", "w", encoding="utf-8") as f:
                    f.write(message["text"])

            elif message.get("type") == "image_url":
                image_url = message["image_url"]["url"]
                if image_url.startswith("data:image"):
                    base64_data = image_url.split(",")[1]
                    content_parts.append(
                        Part(inline_data={"mime_type": "image/jpeg", "data": base64_data})
                    )

                    # ç”»åƒã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                    ImageProcessor.save_base64_image(
                        base64_data, query_dir / f"message_{i}_image.jpg"
                    )

        response = self.client.models.generate_content(model=model, contents=content_parts)

        return LLMResponse(
            content=response.text,
            model=model,
            provider="gemini",
            timestamp=datetime.now(),
            metadata={
                "candidates": len(response.candidates),
                "query_dir": str(query_dir),
            },
            raw_response=response,
        )


class LLMManager:
    """LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼çµ±ä¸€ç®¡ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self, config: Optional[LLMConfig] = None):
        self.config = config or LLMConfig()
        self.providers = {
            "openai": OpenAIProvider(self.config),
            "anthropic": AnthropicProvider(self.config),
            "gemini": GeminiProvider(self.config),
        }

    def get_provider(self, provider_name: str) -> BaseLLMProvider:
        """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å–å¾—"""
        if provider_name not in self.providers:
            raise ValueError(f"Unsupported provider: {provider_name}")
        return self.providers[provider_name]

    def generate_response(self, provider: str, prompt: str, **kwargs) -> LLMResponse:
        """çµ±ä¸€ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
        return self.get_provider(provider).generate_response(prompt, **kwargs)

    def generate_image_response(
        self, provider: str, prompt: str, image_path: str, **kwargs
    ) -> LLMResponse:
        """çµ±ä¸€ç”»åƒãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
        return self.get_provider(provider).generate_image_response(prompt, image_path, **kwargs)


# =============================================================================
# ä»¥ä¸‹ã€main.pyã¨ã®å®Œå…¨ãªå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ãƒ¬ã‚¬ã‚·ãƒ¼é–¢æ•°
# =============================================================================

# ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®šã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_global_config = LLMConfig()


def encode_image(image_path):
    """ãƒ¬ã‚¬ã‚·ãƒ¼: ç”»åƒã‚’Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
    return ImageProcessor.encode_image_to_base64(image_path)


def get_base64_encoded_image(image_path):
    """ãƒ¬ã‚¬ã‚·ãƒ¼: ç”»åƒã‚’Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆåˆ¥åï¼‰"""
    return ImageProcessor.encode_image_to_base64(image_path)


def fetch_response(prompt, image_path, model_name="gpt-4o-mini"):
    """ãƒ¬ã‚¬ã‚·ãƒ¼: OpenAI APIã§ç”»åƒè§£æï¼ˆmain.pyã§ä½¿ç”¨ï¼‰"""
    log_with_time(f"start fetch_response {model_name}", level="INFO")

    # ãƒ¢ãƒ‡ãƒ«åã‚’ä¿®æ­£ï¼ˆmain.pyã§ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ï¼‰
    model_name = "gpt-4o-2024-11-20"

    base64_image = encode_image(image_path)

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {_global_config.openai_api_key}",
    }

    payload = {
        "model": model_name,
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"},
                    },
                ],
            }
        ],
        "max_tokens": 4000,
    }

    response = requests.post(
        "https://api.openai.com/v1/chat/completions", headers=headers, json=payload
    )
    return response


def fetch_gemini_response(prompt, image_path, model="gemini-2.5-flash"):
    """ãƒ¬ã‚¬ã‚·ãƒ¼: Geminiç”»åƒå‡¦ç†ï¼ˆmain.pyã§ä½¿ç”¨ï¼‰"""
    log_with_time(f"start fetch_gemini_response with {model}", level="INFO")

    log_with_time("ğŸŒ Using Gemini API", level="INFO")

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    query_dir = f"tmp/api_queries/gemini_{timestamp}"
    os.makedirs(query_dir, exist_ok=True)

    # ç”»åƒã‚’base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    base64_image = encode_image(image_path)

    # Geminiã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
    client = genai.Client(api_key=_global_config.google_api_key)

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¿å­˜
    with open(f"{query_dir}/prompt.txt", "w", encoding="utf-8") as f:
        f.write(prompt)

    # å…ƒç”»åƒã‚’ã‚³ãƒ”ãƒ¼
    image_filename = os.path.basename(image_path)
    shutil.copy2(image_path, f"{query_dir}/{image_filename}")

    # ã‚¯ã‚¨ãƒªæƒ…å ±ã‚’ä¿å­˜
    query_info = {
        "model": model,
        "timestamp": timestamp,
        "prompt_length": len(prompt),
        "image_path": image_path,
        "image_filename": image_filename,
        "base64_length": len(base64_image),
    }
    with open(f"{query_dir}/query_info.json", "w", encoding="utf-8") as f:
        json.dump(query_info, f, ensure_ascii=False, indent=2)

    log_with_time(f"Query saved to: {query_dir}", level="INFO")

    # ç”»åƒä»˜ããƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆï¼ˆPartã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½¿ç”¨ï¼‰
    contents = [
        Part(text=prompt),
        Part(inline_data={"mime_type": "image/jpeg", "data": base64_image}),
    ]

    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—
    response = client.models.generate_content(model=model, contents=contents)

    return response


def fetch_gemini_response_base64(
    prompt, base64_image, model="gemini-2.5-flash", original_image_path=None
):
    """ãƒ¬ã‚¬ã‚·ãƒ¼: Gemini Base64ç”»åƒå‡¦ç†ï¼ˆmain.pyã§ä½¿ç”¨ï¼‰"""
    log_with_time(f"start fetch_gemini_response_base64 with {model}", level="INFO")

    log_with_time("ğŸŒ Using Gemini API", level="INFO")

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    query_dir = f"tmp/api_queries/gemini_base64_{timestamp}"
    os.makedirs(query_dir, exist_ok=True)

    # Geminiã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
    client = genai.Client(api_key=_global_config.google_api_key)

    # base64_imageãŒdata:image/jpeg;base64,ã§å§‹ã¾ã‚‹å ´åˆã¯å‰Šé™¤
    if base64_image.startswith("data:image"):
        base64_image = base64_image.split(",")[1]

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¿å­˜
    with open(f"{query_dir}/prompt.txt", "w", encoding="utf-8") as f:
        f.write(prompt)

    # ç”»åƒã‚’ä¿å­˜ï¼ˆBase64ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦ä¿å­˜ï¼‰
    import base64 as b64

    image_data = b64.b64decode(base64_image)
    with open(f"{query_dir}/image.jpg", "wb") as f:
        f.write(image_data)

    # ã‚¯ã‚¨ãƒªæƒ…å ±ã‚’ä¿å­˜
    query_info = {
        "model": model,
        "timestamp": timestamp,
        "prompt_length": len(prompt),
        "base64_length": len(base64_image),
        "image_size": len(image_data),
        "original_image_path": original_image_path,
    }
    with open(f"{query_dir}/query_info.json", "w", encoding="utf-8") as f:
        json.dump(query_info, f, ensure_ascii=False, indent=2)

    log_with_time(f"Query saved to: {query_dir}", level="INFO")

    # ç”»åƒä»˜ããƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆï¼ˆPartã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½¿ç”¨ï¼‰
    contents = [
        Part(text=prompt),
        Part(inline_data={"mime_type": "image/jpeg", "data": base64_image}),
    ]

    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—
    response = client.models.generate_content(model=model, contents=contents)

    return response


def fetch_gemini_response_multimodal(prompt_messages, model="gemini-2.5-flash"):
    """ãƒ¬ã‚¬ã‚·ãƒ¼: Gemini APIã§é…åˆ—å½¢å¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡¦ç†ï¼ˆmain.pyã§ä½¿ç”¨ï¼‰"""
    log_with_time(f"start fetch_gemini_response_multimodal with {model}", level="INFO")
    log_with_time(f"Processing {len(prompt_messages)} messages", level="INFO")

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    query_dir = f"tmp/api_queries/gemini_multimodal_{timestamp}"
    os.makedirs(query_dir, exist_ok=True)

    # ã‚¯ã‚¨ãƒªæƒ…å ±ã‚’ä¿å­˜
    query_info = {
        "model": model,
        "timestamp": timestamp,
        "message_count": len(prompt_messages),
        "messages": [],
    }

    # Geminiã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
    client = genai.Client(api_key=_global_config.google_api_key)

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’Geminiã®å½¢å¼ã«å¤‰æ›
    content_parts = []

    for i, message in enumerate(prompt_messages):
        if message.get("type") == "text":
            # ãƒ†ã‚­ã‚¹ãƒˆã®Partã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
            text_content = message["text"]
            content_parts.append(Part(text=text_content))
            log_with_time(f"Message {i}: text ({len(text_content)} chars)", level="INFO")

            # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            text_file = f"{query_dir}/message_{i}_text.txt"
            with open(text_file, "w", encoding="utf-8") as f:
                f.write(text_content)

            query_info["messages"].append(
                {
                    "index": i,
                    "type": "text",
                    "file": f"message_{i}_text.txt",
                    "length": len(text_content),
                }
            )

        elif message.get("type") == "image_url":
            # image_urlã‹ã‚‰ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            image_url = message["image_url"]["url"]
            if image_url.startswith("data:image"):
                # "data:image/jpeg;base64," ã®éƒ¨åˆ†ã‚’é™¤å»
                base64_data = image_url.split(",")[1]
                # ç”»åƒã®Partã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
                content_parts.append(
                    Part(inline_data={"mime_type": "image/jpeg", "data": base64_data})
                )
                log_with_time(
                    f"Message {i}: image (base64, {len(base64_data)} chars)",
                    level="INFO",
                )

                # ç”»åƒã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆBase64ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦ä¿å­˜ï¼‰
                import base64 as b64

                image_data = b64.b64decode(base64_data)
                image_file = f"{query_dir}/message_{i}_image.jpg"
                with open(image_file, "wb") as f:
                    f.write(image_data)

                query_info["messages"].append(
                    {
                        "index": i,
                        "type": "image",
                        "file": f"message_{i}_image.jpg",
                        "base64_length": len(base64_data),
                        "file_size": len(image_data),
                    }
                )

            else:
                log_with_time(
                    f"Warning: Unsupported image URL format: {image_url[:50]}...",
                    level="WARNING",
                )
        else:
            log_with_time(
                f"Warning: Unsupported message type: {message.get('type')}",
                level="WARNING",
            )

    # ã‚¯ã‚¨ãƒªæƒ…å ±ã‚’JSONã§ä¿å­˜
    with open(f"{query_dir}/query_info.json", "w", encoding="utf-8") as f:
        json.dump(query_info, f, ensure_ascii=False, indent=2)

    log_with_time(f"Generated {len(content_parts)} content parts for Gemini", level="INFO")
    log_with_time(f"Query saved to: {query_dir}", level="INFO")

    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—
    try:
        response = client.models.generate_content(model=model, contents=content_parts)
        return response
    except Exception as e:
        log_with_time(f"Error in Gemini API call: {e}", level="ERROR")
        log_with_time(f"Content parts: {len(content_parts)} items", level="ERROR")
        for i, part in enumerate(content_parts):
            log_with_time(f"  Part {i}: {type(part)}", level="ERROR")
            if hasattr(part, "text"):
                log_with_time(f"    - text: {len(part.text)} chars", level="ERROR")
            if hasattr(part, "inline_data"):
                log_with_time(
                    f"    - inline_data: {part.inline_data.keys() if isinstance(part.inline_data, dict) else 'present'}",
                    level="ERROR",
                )
        raise


def fetch_gemini_cli_response(prompt, model="gemini-2.5-flash"):
    """ãƒ¬ã‚¬ã‚·ãƒ¼: Gemini APIã§CLIã‹ã‚‰ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å‡¦ç†ï¼ˆmain.pyã§ä½¿ç”¨ï¼‰"""
    log_with_time(f"start fetch_gemini_cli_response with {model}", level="INFO")
    client = genai.Client(api_key=_global_config.google_api_key)
    response = client.models.generate_content(model=model, contents=prompt)
    return response


def fetch_gemini_4koma_response(prompt, image_path_list, image_data, model="gemini-2.5-flash"):
    """ãƒ¬ã‚¬ã‚·ãƒ¼: Gemini APIã§4ã‚³ãƒå…¨ä½“ã‚’å‡¦ç†ï¼ˆmain.pyã§ä½¿ç”¨ï¼‰"""
    log_with_time(f"start fetch_gemini_4koma_response with {model}", level="INFO")

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    query_dir = f"tmp/api_queries/gemini_4koma_{timestamp}"
    os.makedirs(query_dir, exist_ok=True)

    # Geminiã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
    client = genai.Client(api_key=_global_config.google_api_key)

    # è¤‡æ•°ç”»åƒã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆï¼ˆPartã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½¿ç”¨ï¼‰
    contents = []

    # ã‚¯ã‚¨ãƒªæƒ…å ±ã‚’ä¿å­˜
    query_info = {
        "model": model,
        "timestamp": timestamp,
        "prompt": prompt,
        "image_count": len(image_path_list),
        "images": [],
    }

    # å„ç”»åƒã‚’è¿½åŠ 
    for i, image_path in enumerate(image_path_list):
        contents.append(Part(text=f"Image {i + 1}:"))
        base64_image = get_base64_encoded_image("public" + image_path)
        contents.append(Part(inline_data={"mime_type": "image/jpeg", "data": base64_image}))

        # ç”»åƒã‚’ä¿å­˜ï¼ˆBase64ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦ä¿å­˜ï¼‰
        import base64 as b64

        image_data_bytes = b64.b64decode(base64_image)
        image_file = f"{query_dir}/image_{i + 1}.jpg"
        with open(image_file, "wb") as f:
            f.write(image_data_bytes)

        query_info["images"].append(
            {
                "index": i + 1,
                "path": image_path,
                "file": f"image_{i + 1}.jpg",
                "size": len(image_data_bytes),
            }
        )

    # imageDataã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ 
    contents.append(Part(text=json.dumps(image_data)))
    contents.append(Part(text=prompt))

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¿å­˜
    with open(f"{query_dir}/prompt.txt", "w", encoding="utf-8") as f:
        f.write(prompt)

    # imageDataã‚’ä¿å­˜
    with open(f"{query_dir}/image_data.json", "w", encoding="utf-8") as f:
        json.dump(image_data, f, ensure_ascii=False, indent=2)

    # ã‚¯ã‚¨ãƒªæƒ…å ±ã‚’ä¿å­˜
    with open(f"{query_dir}/query_info.json", "w", encoding="utf-8") as f:
        json.dump(query_info, f, ensure_ascii=False, indent=2)

    log_with_time(f"Query saved to: {query_dir}", level="INFO")

    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—
    response = client.models.generate_content(model=model, contents=contents)

    return response


def fetch_gemini_discussion_response(prompt, summary, image_data, model="gemini-2.5-flash"):
    """ãƒ¬ã‚¬ã‚·ãƒ¼: Gemini APIã§ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³å‡¦ç†ï¼ˆmain.pyã§ä½¿ç”¨ï¼‰"""
    log_with_time(f"start fetch_gemini_discussion_response with {model}", level="INFO")

    # Geminiã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
    client = genai.Client(api_key=_global_config.google_api_key)

    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ§‹ç¯‰
    contents = []

    # summaryãŒãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆã¨ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆã®ä¸¡æ–¹ã«å¯¾å¿œ
    if isinstance(summary, str):
        summary_text = summary
    elif isinstance(summary, dict) and "content" in summary:
        # Claudeã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã®å ´åˆ
        summary_text = (
            summary["content"][0]["text"]
            if isinstance(summary["content"], list)
            else summary["content"]
        )
    else:
        summary_text = json.dumps(summary)

    # ä¼šè©±ã®æµã‚Œã‚’æ§‹ç¯‰
    contents.append(Part(text="ä»¥ä¸‹ã®4ã‚³ãƒæ¼«ç”»ã«ã¤ã„ã¦è©±ã—åˆã„ã¾ã—ã‚‡ã†ã€‚"))
    contents.append(Part(text=f"ç”»åƒãƒ‡ãƒ¼ã‚¿: {json.dumps(image_data)}"))
    contents.append(Part(text=f"4ã‚³ãƒå…¨ä½“ã®ã¾ã¨ã‚: {summary_text}"))
    contents.append(Part(text=f"è³ªå•: {prompt}"))

    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—
    response = client.models.generate_content(model=model, contents=contents)

    return response


def fetch_anthropic_4koma_response(prompt, image_path_list, image_data):
    """ãƒ¬ã‚¬ã‚·ãƒ¼: Claude APIã§4ã‚³ãƒå…¨ä½“ã‚’å‡¦ç†ï¼ˆmain.pyã§ä½¿ç”¨ï¼‰"""
    MODEL_NAME = "claude-3-5-sonnet-20240620"
    log_with_time(f"start fetch_anthropic_4koma_response {MODEL_NAME}", level="INFO")
    message_list = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "Image 1:"},
                {
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": "image/jpeg",
                        "data": get_base64_encoded_image("public" + image_path_list[0]),
                    },
                },
                {"type": "text", "text": "Image 2:"},
                {
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": "image/jpeg",
                        "data": get_base64_encoded_image("public" + image_path_list[1]),
                    },
                },
                {"type": "text", "text": "Image 3:"},
                {
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": "image/jpeg",
                        "data": get_base64_encoded_image("public" + image_path_list[2]),
                    },
                },
                {"type": "text", "text": "Image 4:"},
                {
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": "image/jpeg",
                        "data": get_base64_encoded_image("public" + image_path_list[3]),
                    },
                },
                {"type": "text", "text": json.dumps(image_data)},
                {"type": "text", "text": prompt},
            ],
        },
    ]
    client = anthropic.Anthropic(
        api_key=_global_config.anthropic_api_key,
    )
    response = client.messages.create(model=MODEL_NAME, max_tokens=4048, messages=message_list)
    return response


def fetch_anthropic_discussion_response(prompt, summary, image_data):
    """ãƒ¬ã‚¬ã‚·ãƒ¼: Claude APIã§ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³å‡¦ç†ï¼ˆmain.pyã§ä½¿ç”¨ï¼‰"""
    MODEL_NAME = "claude-3-5-sonnet-20240620"
    log_with_time(f"start fetch_anthropic_discussion_response {MODEL_NAME}", level="INFO")
    assistant_data = {}
    assistant_data = {k: v for k, v in summary.items() if k in ["role", "content"]}

    message_list = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": json.dumps(image_data)},
                {
                    "type": "text",
                    "text": "ç”»åƒéƒ¡ã¨imageDataã‹ã‚‰4ã‚³ãƒå…¨ä½“ã§ã©ã‚“ãªè©±ã‹ã‚’ã¾ã¨ã‚ã¦ãã ã•ã„",
                },
            ],
        },
        assistant_data,
        {"role": "user", "content": [{"type": "text", "text": prompt}]},
    ]
    client = anthropic.Anthropic(
        api_key=_global_config.anthropic_api_key,
    )
    response = client.messages.create(model=MODEL_NAME, max_tokens=4048, messages=message_list)
    return response


def get_gpt_response(prompt: str, image_path: str, model_name: str = "gpt-4o-2024-11-20") -> Dict:
    """ãƒ¬ã‚¬ã‚·ãƒ¼: GPTãƒ¬ã‚¹ãƒãƒ³ã‚¹å–å¾—ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰"""
    manager = LLMManager()
    response = manager.generate_image_response("openai", prompt, image_path, model=model_name)
    return {"choices": [{"message": {"content": response.content}}]}


def get_claude_response(prompt: str, image_path: str = None) -> Dict:
    """ãƒ¬ã‚¬ã‚·ãƒ¼: Claudeãƒ¬ã‚¹ãƒãƒ³ã‚¹å–å¾—ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰"""
    manager = LLMManager()
    if image_path:
        response = manager.generate_image_response("anthropic", prompt, image_path)
    else:
        response = manager.generate_response("anthropic", prompt)
    return {"content": [{"text": response.content}]}


def get_gemini_response(prompt: str, model: str = "gemini-2.5-flash") -> str:
    """ãƒ¬ã‚¬ã‚·ãƒ¼: Geminiãƒ¬ã‚¹ãƒãƒ³ã‚¹å–å¾—ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰"""
    manager = LLMManager()
    response = manager.generate_response("gemini", prompt, model=model)
    return response.content


def print_and_save_response(response, koma_id):
    """ãƒ¬ã‚¬ã‚·ãƒ¼: ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®è¡¨ç¤ºã¨ä¿å­˜"""
    save_dir = "chat_results"
    os.makedirs(name=save_dir, exist_ok=True)
    log_with_time(f"Response attributes: {dir(response)}", level="DEBUG")
    log_with_time(f"Response content: {response.json()['content'][0]['text']}", level="DEBUG")

    with open(save_dir + "/" + koma_id + ".json", "w") as f:
        f.write(response.text)


def save_response(response, image_path):
    """ãƒ¬ã‚¬ã‚·ãƒ¼: ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½è¨˜"""
    p = Path(image_path)
    with open(f"chat_results/result.txt", "+a") as f:
        f.write(f"{image_path}___{response.json()}\n")


def build_koma_id(image_path):
    """ãƒ¬ã‚¬ã‚·ãƒ¼: ç”»åƒãƒ‘ã‚¹ã‹ã‚‰ã‚³ãƒIDã‚’ç”Ÿæˆ"""
    data_name = image_path.split("/")[1]
    kanji = data_name[-2:]
    page_koma_num = RE_PAGE_KOMA_NUM.findall(image_path)[-1]
    koma_id = f"{kanji}-{page_koma_num}"
    return koma_id


# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
prompt_anal_koma = """
ã€Œã‚ã‹ã‚‰ãªã„ã“ã¨ã¯çŸ¥ã‚‰ãªã„ã¨ç­”ãˆã¦ã€
ä»¥ä¸‹ã¯ã€ã‚†ã‚†å¼ã€ã®ç”»åƒã§ã™ã€‚æ—¥æœ¬ã®4ã‚³ãƒæ¼«ç”»ã®ãŸã‚ã€å³ã‹ã‚‰å·¦ã«èª­ã‚“ã§ã„ãã¾ã™ã€‚ä¸»ãªç™»å ´äººç‰©ã¯ä»¥ä¸‹ã§ã™ã€‚

- æ—¥å‘ç¸
	- "ç¸ã®é«ªã®ç‰¹å¾´": [  "é»’ã£ã½ã„é«ªè‰²",  "é•·ã‚ï¼ˆè‚©ãã‚‰ã„ã®é•·ã•ï¼‰",  "ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆãªé«ªè³ª",  "å‰é«ªãŒã‚ã‚‹"  ]
- æ«Ÿäº•å”¯ 
	- "å”¯ã®é«ªã®ç‰¹å¾´": [  "ç™½ã£ã½ã„é«ªè‰²",  "çŸ­ã‚ï¼ˆé¦–ã‚ãŸã‚Šã®é•·ã•ï¼‰",  "ã‚„ã‚„ã‚¦ã‚§ãƒ¼ãƒ–ãŒã‹ã‹ã£ã¦ã„ã‚‹ã‚ˆã†ã«è¦‹ãˆã‚‹",  "å‰é«ªãŒã‚ã‚‹",  "é«ªã®å…ˆç«¯ãŒå°‘ã—å¤–å´ã«è·³ã­ã¦ã„ã‚‹"  ]
- é‡ã€…åŸã‚†ãšã“ 
	- "ã‚†ãšã“ã®é«ªã®ç‰¹å¾´": [ "çŸ­ã‚",  "ãƒœãƒ–ã‚«ãƒƒãƒˆã®ã‚ˆã†ãªã‚¹ã‚¿ã‚¤ãƒ«",  "å‰é«ªãŒã‚„ã‚„é•·ã‚",  "é«ªã®å…ˆç«¯ãŒå†…å´ã«å°‘ã—å·»ãè¾¼ã‚“ã§ã„ã‚‹",  "é«ªè‰²ã¯ç™½é«ªã§ã‚‚é»’é«ªã§ã‚‚ãªã„ä¸­é–“çš„ãªè‰²ï¼ˆãŠãã‚‰ãã‚°ãƒ¬ãƒ¼ãŒã‹ã£ãŸè‰²ï¼‰"  ]
ã“ã®ç”»åƒã«ã¤ã„ã¦ä»¥ä¸‹ã«ç­”ãˆã¦ãã ã•ã„ã€‚ãã‚Œãã‚Œã«ã¤ã„ã¦ç”»åƒã®å³å´ã«ã‚ã‚‹ã‚‚ã®ã‹ã‚‰é †ã«ç­”ãˆã¦ãã ã•ã„ã€‚
- ã‚†ãšã“ãŸã¡ã¯ã€ãã‚Œãã‚ŒãŒã‚³ãƒã®ã©ã“ã«ã„ã¾ã™ã‹ï¼Ÿèª°ã‚‚ã„ãªã„å ´åˆã‚‚ã‚ã‚‹ã—ã€3äººã‚ˆã‚Šå¤šã„äººæ•°ãŒå†™ã£ã¦ã„ã‚‹å ´åˆã‚‚ã‚ã‚Šã¾ã™
	- ç”»åƒã®å³å´ã«ã‚ã‚‹ã‚‚ã®ã‹ã‚‰é †ã«ç­”ãˆã¦ãã ã•ã„ã€‚
	- åº§æ¨™ã‚‚ç­”ãˆã¦ãã ã•ã„
- ã©ã‚“ãªè¡¨æƒ…ã‚’ã—ã¦ã„ã¾ã™ã‹?ã„ãªã„å ´åˆã¯ã€Œã„ãªã„ã€
- é¡”ã®å‘ãã‚’ç­”ãˆã¦ãã ã•ã„.ã„ãªã„å ´åˆã¯ã€Œã„ãªã„ã€
- ã©ã‚“ãªã‚»ãƒªãƒ•ã‚’è¨€ã£ã¦ã„ã¾ã™ã‹ï¼Ÿãªã„å ´åˆã¯ã€Œãªã—ã€
	- ç”»åƒã®å³å´ã«ã‚ã‚‹ã‚‚ã®ã‹ã‚‰é †ã«ç­”ãˆã¦ãã ã•ã„ã€‚
	- åº§æ¨™ã‚‚ç”»åƒã«å¯¾ã™ã‚‹æ¯”ç‡ã§ç­”ãˆã¦ãã ã•ã„ ã§ç­”ãˆã¦ãã ã•ã„ 
	- æ–‡ãŒè¤‡æ•°ã‚ã‚‹å ´åˆã¯åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã§åŒºåˆ‡ã£ã¦ãã ã•ã„
- ã©ã‚“ãªæœè£…ã‚’ã—ã¦ã„ã¾ã™ã‹ï¼Ÿã„ãªã„å ´åˆã¯ã€Œã„ãªã„ã€
- ã©ã‚“ãªå ´é¢ã ã¨æ€ã‚ã‚Œã¾ã™ã‹ï¼Ÿã„ãªã„å ´åˆã¯ã€Œã„ãªã„ã€
- è©±ã—ã¦ã„ã‚‹å ´æ‰€ã¯ã©ã“ã§ã™ã‹ï¼Ÿã‚ã‹ã‚‰ãªã„å ´åˆã¯ã€Œä¸æ˜ã€
- èƒŒæ™¯ã«æ¼«ç”»ãªã‚‰ã§ã¯ã®åŠ¹æœæå†™ãŒã‚ã‚Œã°æ•™ãˆã¦ä¸‹ã•ã„ã€‚ãªã„å ´åˆã¯ã€Œãªã—ã€
- ç”»åƒã®ä¸­ã«ã¯å…¨éƒ¨ã§ä½•äººã„ã¾ã™ã‹
- ç”»åƒã®ä¸­ã«ã¯å¹ãå‡ºã—ãŒã„ãã¤ã‚ã‚Šã¾ã™ã‹
- å‡ºåŠ›ã¯JSONå½¢å¼ã®ã¿

å‡ºåŠ›ã™ã‚‹jsonã¯ä»¥ä¸‹ã®å½¢å¼ã«ã—ã¦ãã ã•ã„
```
{
    charactersNum: int,
    serifsNum: int,
    characters: [
    {
        character: "",
        faceDirection: "",
        position: "",
        expression: "",
        serif: "",
        clothing: "",
        isVisible: true,
    },
    {
        character: "",
        faceDirection: "",
        position: "",
        expression: "",
        serif: "",
        clothing: "",
        isVisible: true,
    },
    {
        character: "",
        faceDirection: "",
        position: "",
        expression: "",
        serif: "",
        clothing: "",
        isVisible: true,
    },
    ],
    sceneData: {scene: "", location: "", backgroundEffects: ""},
}
```
"""


# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆã‚¯ãƒ©ã‚¹ç‰ˆï¼‰
class PromptTemplates:
    """4ã‚³ãƒæ¼«ç”»è§£æç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ"""

    KOMA_ANALYSIS = prompt_anal_koma


def save_response_to_file(response: LLMResponse, output_path: str) -> None:
    """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(
            {
                "content": response.content,
                "model": response.model,
                "provider": response.provider,
                "timestamp": response.timestamp.isoformat(),
                "metadata": response.metadata,
            },
            f,
            ensure_ascii=False,
            indent=2,
        )
